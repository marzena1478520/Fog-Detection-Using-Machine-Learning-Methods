{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3efc4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pypots.utils.random import set_random_seed\n",
    "from pypots.optim import Adam\n",
    "from pypots.classification import Raindrop, BRITS, GRUD\n",
    "from pypots.nn.functional import calc_binary_classification_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b25e2",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    cols = [\n",
    "        \"Temperature_C\",\n",
    "        \"Dew_Point_C\",\n",
    "        \"Humidity_%\",\n",
    "        \"Wind_Speed_kmh\",\n",
    "        \"Wind_Gust_kmh\",\n",
    "        \"Pressure_hPa\",\n",
    "        \"Precip_Rate_mm\"\n",
    "    ]\n",
    "\n",
    "    # Convert to float\n",
    "    df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Convert Wind and Condition to categorical\n",
    "    df[\"Wind\"] = df[\"Wind\"].astype(\"category\")\n",
    "    df[\"Condition\"] = df[\"Condition\"].astype(\"category\")\n",
    "\n",
    "    # join date and time\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n",
    "    df = df.set_index(\"datetime\")\n",
    "\n",
    "    # dtop Date and Time\n",
    "    df = df.drop(columns=[\"Date\", \"Time\"])\n",
    "\n",
    "\n",
    "    # Count duplicates on datetime\n",
    "    df.index.duplicated().sum()\n",
    "    # Drop duplicates\n",
    "    df = df[~df.index.duplicated()]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bc73e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_sequences_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    sequence_length: int,\n",
    "    target_column: str,\n",
    "    target_keyword: str | None,\n",
    "    numerical_cols: list[str],\n",
    "    categorical_cols_encoders: dict[str, OneHotEncoder],\n",
    "    master_feature_columns: list[str],\n",
    "    nan_placeholder: str = 'missing_value',\n",
    "    step_size: int = 1\n",
    "):\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # 1. Target variable processing\n",
    "    if target_column not in df_copy.columns:\n",
    "        print(f\"Warning: Target column '{target_column}' not found in DataFrame. Skipping sequence generation for this DF.\")\n",
    "        return None\n",
    "    \n",
    "    if target_keyword is not None:\n",
    "        y_series = df_copy[target_column].astype(str).str.contains(target_keyword, case=False, na=False).astype(int)\n",
    "    else: # Handle cases where target_keyword is None (e.g., target is already binary)\n",
    "        if pd.api.types.is_bool_dtype(df_copy[target_column]):\n",
    "            y_series = df_copy[target_column].astype(int)\n",
    "        elif pd.api.types.is_numeric_dtype(df_copy[target_column]) and df_copy[target_column].dropna().isin([0, 1]).all():\n",
    "            y_series = df_copy[target_column].fillna(-1).astype(int) # FillNa if necessary, then convert. -1 for missing y? Or drop?\n",
    "            y_series = y_series[y_series != -1] # Drop rows where target was NaN\n",
    "            if y_series.empty and not df_copy[target_column].dropna().empty : # if all targets were NaN\n",
    "                 print(f\"Warning: All values in target column '{target_column}' were NaN after attempting to treat as binary. Skipping.\")\n",
    "                 return None\n",
    "        else:\n",
    "            print(f\"Error: target_keyword is None, but target_column '{target_column}' is not boolean or binary numeric. Cannot process target.\")\n",
    "            return None\n",
    "    \n",
    "    # Adjust df_copy if rows were dropped from y_series due to NaNs in target\n",
    "    if len(y_series) < len(df_copy):\n",
    "        df_copy = df_copy.loc[y_series.index]\n",
    "        if df_copy.empty:\n",
    "            return None\n",
    "\n",
    "\n",
    "    # 2. Feature Processing\n",
    "    processed_features_list = []\n",
    "\n",
    "    # Numerical features\n",
    "    present_numerical_cols = [col for col in numerical_cols if col in df_copy.columns]\n",
    "    if present_numerical_cols:\n",
    "        processed_features_list.append(df_copy[present_numerical_cols].copy())\n",
    "    else:\n",
    "        # If no numerical columns are present but some were expected, create an empty DF with original index\n",
    "        # This helps pd.concat and reindex later if only categorical features exist.\n",
    "        if numerical_cols: # only if numerical_cols list was not empty\n",
    "             processed_features_list.append(pd.DataFrame(index=df_copy.index))\n",
    "\n",
    "\n",
    "    # Categorical features\n",
    "    for cat_col_name, encoder in categorical_cols_encoders.items():\n",
    "        if cat_col_name in df_copy.columns:\n",
    "            cat_column_data = df_copy[[cat_col_name]].copy()\n",
    "            cat_column_data[cat_col_name] = cat_column_data[cat_col_name].astype(object).fillna(nan_placeholder)\n",
    "            \n",
    "            try:\n",
    "                transformed_data = encoder.transform(cat_column_data)\n",
    "                feature_names = encoder.get_feature_names_out([cat_col_name])\n",
    "                transformed_df = pd.DataFrame(transformed_data, columns=feature_names, index=df_copy.index)\n",
    "                processed_features_list.append(transformed_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error transforming categorical column {cat_col_name} with pre-fitted encoder: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: Categorical column '{cat_col_name}' (for pre-fitted encoder) not found in current DataFrame.\")\n",
    "            # We still need to account for its feature columns in the master list.\n",
    "            # Create empty columns for this encoder if it's missing, using its expected feature names.\n",
    "            try:\n",
    "                missing_encoder_feature_names = encoder.get_feature_names_out([cat_col_name])\n",
    "                missing_df = pd.DataFrame(0, index=df_copy.index, columns=missing_encoder_feature_names)\n",
    "                processed_features_list.append(missing_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not get feature names for missing {cat_col_name} to create placeholders: {e}\")\n",
    "\n",
    "\n",
    "    if not processed_features_list:\n",
    "        # This case means no numerical cols were selected AND no categorical cols processed.\n",
    "        # Reindex will handle creating an all-zero feature matrix if master_feature_columns is not empty.\n",
    "        combined_features_df = pd.DataFrame(index=df_copy.index) \n",
    "    else:\n",
    "        combined_features_df = pd.concat(processed_features_list, axis=1)\n",
    "    \n",
    "    aligned_features_df = combined_features_df.reindex(columns=master_feature_columns, fill_value=0.0)\n",
    "    X_values = aligned_features_df.astype(np.float32).values\n",
    "\n",
    "    # 3. Sequence Generation\n",
    "    X_sequences = []\n",
    "    y_labels_for_sequences = []\n",
    "    num_rows_X = len(X_values)\n",
    "    num_rows_y = len(y_series)\n",
    "\n",
    "    # Ensure X and y have aligned lengths after any potential filtering\n",
    "    if num_rows_X == 0 or num_rows_y == 0: # No data to form sequences\n",
    "        return None\n",
    "    if num_rows_X != num_rows_y: # Should not happen if df_copy.loc[y_series.index] was effective\n",
    "        print(f\"Warning: Mismatch in lengths of X ({num_rows_X}) and y ({num_rows_y}) after processing. Skipping sequences for this DF.\")\n",
    "        return None\n",
    "    \n",
    "    if num_rows_X < sequence_length:\n",
    "        return None\n",
    "\n",
    "    for i in range(0, num_rows_X - sequence_length + 1, step_size):\n",
    "        feature_seq = X_values[i : i + sequence_length]\n",
    "        X_sequences.append(feature_seq)\n",
    "        # y_series was already aligned with df_copy, which X_values is based on\n",
    "        label = y_series.iloc[i + sequence_length - 1]\n",
    "        y_labels_for_sequences.append(label)\n",
    "\n",
    "    if not X_sequences:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"X\": np.array(X_sequences, dtype=np.float32),\n",
    "        \"y\": np.array(y_labels_for_sequences, dtype=np.int32)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1bd96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(\n",
    "    train_paths: list[str],\n",
    "    test_paths: list[str],\n",
    "    prepare_df_function: callable,\n",
    "    sequence_length: int,\n",
    "    target_column: str,\n",
    "    target_keyword: str | None,\n",
    "    numerical_cols: list[str],\n",
    "    categorical_cols_encoders: dict[str, OneHotEncoder],\n",
    "    nan_placeholder: str = 'missing_value',\n",
    "    step_size: int = 1\n",
    "):\n",
    "    master_feature_columns = list(numerical_cols)\n",
    "    \n",
    "    for cat_col_name in categorical_cols_encoders:\n",
    "        encoder = categorical_cols_encoders[cat_col_name]\n",
    "        try:\n",
    "            master_feature_columns.extend(encoder.get_feature_names_out([cat_col_name]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting feature names from pre-fitted encoder for '{cat_col_name}': {e}. \"\n",
    "                  \"Ensure encoders are fitted and can produce feature names. Feature set might be incomplete.\")\n",
    "\n",
    "    if not master_feature_columns:\n",
    "        print(\"Warning: Master feature column list is empty. No features defined by numerical_cols or encoders.\")\n",
    "        empty_X_shape = (0, sequence_length, 0) if sequence_length > 0 else (0,0,0)\n",
    "        empty_result = {\"X\": np.array([]).reshape(empty_X_shape), \"y\": np.array([])}\n",
    "        return empty_result, empty_result\n",
    "    \n",
    "    print(f\"Master feature columns determined ({len(master_feature_columns)} total). Example: {master_feature_columns[:5]}...\")\n",
    "\n",
    "    def _bulk_process_paths_with_prefitted_encoders(\n",
    "        file_paths: list[str],\n",
    "        is_training_data: bool\n",
    "    ):\n",
    "        all_X_sequences_list = []\n",
    "        all_y_labels_list = []\n",
    "        \n",
    "        dataset_type = \"training\" if is_training_data else \"testing\"\n",
    "        print(f\"\\nProcessing {dataset_type} data...\")\n",
    "\n",
    "        for i, path in enumerate(file_paths):\n",
    "            print(f\"  Loading and preparing {dataset_type} file {i+1}/{len(file_paths)}: {path}\")\n",
    "            try:\n",
    "                df = prepare_df_function(path)\n",
    "            except Exception as e:\n",
    "                print(f\"    Error calling prepare_df_function for {path}: {e}. Skipping this file.\")\n",
    "                continue\n",
    "\n",
    "            if df is None or df.empty:\n",
    "                print(f\"    prepare_df_function returned None or empty DataFrame for {path}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            pypots_dict_single_df = _generate_sequences_from_df(\n",
    "                df=df,\n",
    "                sequence_length=sequence_length,\n",
    "                target_column=target_column,\n",
    "                target_keyword=target_keyword,\n",
    "                numerical_cols=numerical_cols,\n",
    "                categorical_cols_encoders=categorical_cols_encoders,\n",
    "                master_feature_columns=master_feature_columns,\n",
    "                nan_placeholder=nan_placeholder,\n",
    "                step_size=step_size\n",
    "            )\n",
    "\n",
    "            if pypots_dict_single_df and pypots_dict_single_df['X'].size > 0:\n",
    "                all_X_sequences_list.append(pypots_dict_single_df['X'])\n",
    "                all_y_labels_list.append(pypots_dict_single_df['y'])\n",
    "            else:\n",
    "                print(f\"    No sequences generated for {path}.\")\n",
    "\n",
    "        if not all_X_sequences_list:\n",
    "            print(f\"No sequences generated for any {dataset_type} files.\")\n",
    "            n_master_features = len(master_feature_columns)\n",
    "            empty_X_shape = (0, sequence_length, n_master_features) if sequence_length > 0 else (0,0,0)\n",
    "            return {\"X\": np.array([]).reshape(empty_X_shape), \"y\": np.array([])}\n",
    "\n",
    "        final_X = np.concatenate(all_X_sequences_list, axis=0)\n",
    "        final_y = np.concatenate(all_y_labels_list, axis=0)\n",
    "        return {\"X\": final_X, \"y\": final_y}\n",
    "\n",
    "    # Process training data\n",
    "    train_pypots_data = _bulk_process_paths_with_prefitted_encoders(train_paths, is_training_data=True)\n",
    "    if train_pypots_data[\"X\"].size > 0:\n",
    "        print(f\"Total training data: X shape {train_pypots_data['X'].shape}, y shape {train_pypots_data['y'].shape}\")\n",
    "\n",
    "    # Process testing data\n",
    "    test_pypots_data = _bulk_process_paths_with_prefitted_encoders(test_paths, is_training_data=False)\n",
    "    if test_pypots_data[\"X\"].size > 0:\n",
    "        print(f\"Total testing data: X shape {test_pypots_data['X'].shape}, y shape {test_pypots_data['y'].shape}\")\n",
    "        \n",
    "    return train_pypots_data, test_pypots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5c65cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 12\n",
    "STEP_SIZE = 1\n",
    "TARGET_COLUMN = 'Condition'\n",
    "FOG_KEYWORD = 'fog'\n",
    "CATEGORICAL_FEATURES = ['Wind'] # List of raw categorical column names\n",
    "NAN_PLACEHOLDER = 'missing_value' # For categoricals\n",
    "\n",
    "TRAIN_PATHS = [\n",
    "    \"./datasets/weather/wrocław-EPWR.csv\",\n",
    "    \"./datasets/weather/utrecht-IUTRECHT299.csv\",\n",
    "    \"./datasets/weather/utrecht-IUTRECHT315.csv\",\n",
    "    \"./datasets/weather/utrecht-IDEBIL13.csv\"\n",
    "]\n",
    "\n",
    "TEST_PATHS = [\n",
    "    \"./datasets/weather/utrecht-EHAM.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ca3c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master feature columns determined (7 total). Example: ['Temperature_C', 'Dew_Point_C', 'Humidity_%', 'Wind_Speed_kmh', 'Wind_Gust_kmh']...\n",
      "\n",
      "Processing training data...\n",
      "  Loading and preparing training file 1/4: ./datasets/weather/wrocław-EPWR.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196419/1984567303.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading and preparing training file 2/4: ./datasets/weather/utrecht-IUTRECHT299.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196419/1984567303.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading and preparing training file 3/4: ./datasets/weather/utrecht-IUTRECHT315.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196419/1984567303.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading and preparing training file 4/4: ./datasets/weather/utrecht-IDEBIL13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196419/1984567303.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training data: X shape (46719, 12, 7), y shape (46719,)\n",
      "\n",
      "Processing testing data...\n",
      "  Loading and preparing testing file 1/1: ./datasets/weather/utrecht-EHAM.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196419/1984567303.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total testing data: X shape (11687, 12, 7), y shape (11687,)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = prepare_datasets(\n",
    "    train_paths=TRAIN_PATHS,\n",
    "    test_paths=TEST_PATHS,\n",
    "    prepare_df_function=prepare_df,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    target_keyword=FOG_KEYWORD,\n",
    "    numerical_cols=[\"Temperature_C\",\"Dew_Point_C\",\"Humidity_%\",\"Wind_Speed_kmh\",\"Wind_Gust_kmh\",\"Pressure_hPa\",\"Precip_Rate_mm\"],\n",
    "    categorical_cols_encoders={},\n",
    "    nan_placeholder=NAN_PLACEHOLDER,\n",
    "    step_size=STEP_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "301a6f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training X shape: (46719, 12, 7)\n",
      "Final Training y shape: (46719,)\n",
      "Training y distribution: [45442  1277]\n",
      "Final Test X shape: (11687, 12, 7)\n",
      "Final Test y shape: (11687,)\n",
      "Test y distribution (if not empty): [11400   287]\n"
     ]
    }
   ],
   "source": [
    "if train_data and train_data[\"X\"].size > 0:\n",
    "    print(f\"Final Training X shape: {train_data['X'].shape}\")\n",
    "    print(f\"Final Training y shape: {train_data['y'].shape}\")\n",
    "    print(f\"Training y distribution: {np.bincount(train_data['y'])}\")\n",
    "else:\n",
    "    print(\"Training data is empty or could not be generated.\")\n",
    "\n",
    "if test_data and test_data[\"X\"].size > 0:\n",
    "    print(f\"Final Test X shape: {test_data['X'].shape}\")\n",
    "    print(f\"Final Test y shape: {test_data['y'].shape}\")\n",
    "    print(f\"Test y distribution (if not empty): {np.bincount(test_data['y']) if test_data['y'].size > 0 else 'empty'}\")\n",
    "else:\n",
    "    print(\"Test data is empty or could not be generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f2a96",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f9ac0",
   "metadata": {},
   "source": [
    "## Raindrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efaf9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:07:11 [INFO]: No given device, using default device: cuda\n",
      "2025-05-12 15:07:11 [INFO]: Model files will be saved to ./runs/classify/WEATHER/raindrop/20250512_T150711\n",
      "2025-05-12 15:07:11 [INFO]: Tensorboard file will be saved to ./runs/classify/WEATHER/raindrop/20250512_T150711/tensorboard\n",
      "2025-05-12 15:07:11 [INFO]: Using customized CrossEntropy as the training loss function.\n",
      "2025-05-12 15:07:11 [INFO]: Using customized CrossEntropy as the validation metric function.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "❌ name 'MessagePassing' is not defined. Note that torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/pypots/nn/modules/raindrop/backbone.py:36\u001b[39m, in \u001b[36mBackboneRaindrop.__init__\u001b[39m\u001b[34m(self, n_features, n_layers, d_model, n_heads, d_ffn, n_classes, dropout, max_len, d_static, d_pe, aggregation, sensor_wise_mask, static)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PositionalEncoding, ObservationPropagation\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mNameError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/pypots/nn/modules/raindrop/layers.py:65\u001b[39m\n\u001b[32m     62\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pe\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mObservationPropagation\u001b[39;00m(\u001b[43mMessagePassing\u001b[49m):\n\u001b[32m     66\u001b[39m     _alpha: OptTensor\n",
      "\u001b[31mNameError\u001b[39m: name 'MessagePassing' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m raindrop = \u001b[43mRaindrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_ffn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43msaving_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./runs/classify/WEATHER/raindrop\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_saving_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/pypots/classification/raindrop/model.py:162\u001b[39m, in \u001b[36mRaindrop.__init__\u001b[39m\u001b[34m(self, n_steps, n_features, n_classes, n_layers, d_model, n_heads, d_ffn, dropout, d_static, aggregation, sensor_wise_mask, static, batch_size, epochs, patience, training_loss, validation_metric, optimizer, num_workers, device, saving_path, model_saving_strategy, verbose)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28mself\u001b[39m.n_steps = n_steps\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# set up the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43m_Raindrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_ffn\u001b[49m\u001b[43m=\u001b[49m\u001b[43md_ffn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_static\u001b[49m\u001b[43m=\u001b[49m\u001b[43md_static\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43maggregation\u001b[49m\u001b[43m=\u001b[49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43msensor_wise_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msensor_wise_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidation_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28mself\u001b[39m._send_model_to_given_device()\n\u001b[32m    179\u001b[39m \u001b[38;5;28mself\u001b[39m._print_model_size()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/pypots/classification/raindrop/core.py:48\u001b[39m, in \u001b[36m_Raindrop.__init__\u001b[39m\u001b[34m(self, n_features, n_layers, d_model, n_heads, d_ffn, n_classes, dropout, max_len, d_static, aggregation, sensor_wise_mask, static, training_loss, validation_metric)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mself\u001b[39m.validation_metric = validation_metric\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28mself\u001b[39m.backbone = \u001b[43mBackboneRaindrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_ffn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_static\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_pe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43msensor_wise_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m static:\n\u001b[32m     65\u001b[39m     d_final = d_model + n_features\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/pypots/nn/modules/raindrop/backbone.py:38\u001b[39m, in \u001b[36mBackboneRaindrop.__init__\u001b[39m\u001b[34m(self, n_features, n_layers, d_model, n_heads, d_ffn, n_classes, dropout, max_len, d_static, d_pe, aggregation, sensor_wise_mask, static)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PositionalEncoding, ObservationPropagation\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mNameError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     39\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m❌ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Note that torch_geometric is missing, please install it with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mpip install torch_geometric torch_scatter torch_sparse\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mconda install -c pyg pyg pytorch-scatter pytorch-sparse\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m     )\n\u001b[32m     44\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.n_layers = n_layers\n",
      "\u001b[31mImportError\u001b[39m: ❌ name 'MessagePassing' is not defined. Note that torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'"
     ]
    }
   ],
   "source": [
    "raindrop = Raindrop(\n",
    "    n_steps=train_data['X'].shape[1],\n",
    "    n_features=train_data['X'].shape[2],\n",
    "    n_classes=2,\n",
    "    n_layers=2,\n",
    "    d_model=train_data['X'].shape[2] * 4,\n",
    "    d_ffn=256,\n",
    "    n_heads=2,\n",
    "    dropout=0.3,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    patience=3,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=None,\n",
    "    saving_path='./runs/classify/WEATHER/raindrop',\n",
    "    model_saving_strategy='best'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf20327",
   "metadata": {},
   "source": [
    "## BRITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0847e160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:12:58 [INFO]: No given device, using default device: cuda\n",
      "2025-05-12 15:12:58 [INFO]: Model files will be saved to ./runs/classify/WEATHER/brits/20250512_T151258\n",
      "2025-05-12 15:12:58 [INFO]: Tensorboard file will be saved to ./runs/classify/WEATHER/brits/20250512_T151258/tensorboard\n",
      "2025-05-12 15:12:58 [INFO]: Using customized CrossEntropy as the training loss function.\n",
      "2025-05-12 15:12:58 [INFO]: Using customized CrossEntropy as the validation metric function.\n",
      "2025-05-12 15:13:00 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 566,212\n"
     ]
    }
   ],
   "source": [
    "brits = BRITS(\n",
    "    n_steps=train_data['X'].shape[1],\n",
    "    n_features=train_data['X'].shape[2],\n",
    "    n_classes=2,\n",
    "    rnn_hidden_size=256,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    patience=3,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=None,\n",
    "    saving_path='./runs/classify/WEATHER/brits',\n",
    "    model_saving_strategy='best'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "735b1014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:22:59 [INFO]: Epoch 001 - training loss (CrossEntropy): 286.7109, validation CrossEntropy: 0.1258\n",
      "2025-05-12 15:29:50 [INFO]: Epoch 002 - training loss (CrossEntropy): 229.2128, validation CrossEntropy: 0.1219\n",
      "2025-05-12 15:36:04 [INFO]: Epoch 003 - training loss (CrossEntropy): 183.3828, validation CrossEntropy: 0.1189\n",
      "2025-05-12 15:42:13 [INFO]: Epoch 004 - training loss (CrossEntropy): 138.8626, validation CrossEntropy: 0.1197\n",
      "2025-05-12 15:48:23 [INFO]: Epoch 005 - training loss (CrossEntropy): 97.0802, validation CrossEntropy: 0.1215\n",
      "2025-05-12 15:54:29 [INFO]: Epoch 006 - training loss (CrossEntropy): 68.2444, validation CrossEntropy: 0.1156\n",
      "2025-05-12 16:00:41 [INFO]: Epoch 007 - training loss (CrossEntropy): 53.2646, validation CrossEntropy: 0.1157\n",
      "2025-05-12 16:06:59 [INFO]: Epoch 008 - training loss (CrossEntropy): 39.8244, validation CrossEntropy: 0.1166\n",
      "2025-05-12 16:13:10 [INFO]: Epoch 009 - training loss (CrossEntropy): 34.1167, validation CrossEntropy: 0.1128\n",
      "2025-05-12 16:18:45 [INFO]: Epoch 010 - training loss (CrossEntropy): 33.0141, validation CrossEntropy: 0.1052\n",
      "2025-05-12 16:18:45 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-05-12 16:18:45 [INFO]: Saved the model to ./runs/classify/WEATHER/brits/20250512_T151258/BRITS.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.5, \n",
      "PR_AUC: 0.5122786001540173,\n",
      "F1: 0.0,\n",
      "Precision: 0.0,\n",
      "Recall: 0.0,\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/next/magisterka/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "brits.fit(train_set=train_data, val_set=test_data)\n",
    "results = brits.predict(test_data)\n",
    "prediction = results['classification']\n",
    "metrics = calc_binary_classification_metrics(prediction, test_data['y'])\n",
    "print(\"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
