{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efc4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pypots.utils.random import set_random_seed\n",
    "from pypots.optim import Adam\n",
    "from pypots.classification import Raindrop, BRITS, GRUD\n",
    "from pypots.nn.functional import calc_binary_classification_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b25e2",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    cols = [\n",
    "        \"Temperature_C\",\n",
    "        \"Dew_Point_C\",\n",
    "        \"Humidity_%\",\n",
    "        \"Wind_Speed_kmh\",\n",
    "        \"Wind_Gust_kmh\",\n",
    "        \"Pressure_hPa\",\n",
    "        \"Precip_Rate_mm\"\n",
    "    ]\n",
    "\n",
    "    # Convert to float\n",
    "    df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Convert Wind and Condition to categorical\n",
    "    df[\"Wind\"] = df[\"Wind\"].astype(\"category\")\n",
    "    df[\"Condition\"] = df[\"Condition\"].astype(\"category\")\n",
    "\n",
    "    # join date and time\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n",
    "    df = df.set_index(\"datetime\")\n",
    "\n",
    "    # dtop Date and Time\n",
    "    df = df.drop(columns=[\"Date\", \"Time\"])\n",
    "\n",
    "\n",
    "    # Count duplicates on datetime\n",
    "    df.index.duplicated().sum()\n",
    "    # Drop duplicates\n",
    "    df = df[~df.index.duplicated()]\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_balanced_subsamble(\n",
    "        df: pd.DataFrame, target_column: str, target_values: list[str] = None, seed: int | None = None\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    if target_column not in df.columns:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    df_cleaned = df.dropna(subset=[target_column])\n",
    "\n",
    "    if df_cleaned.empty:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    balanced_samples = []\n",
    "\n",
    "    if target_values is not None:\n",
    "        unique_target_categories = list(set(target_values))\n",
    "\n",
    "        if not unique_target_categories:\n",
    "            return pd.DataFrame(columns=df.columns)\n",
    "        \n",
    "        df_filtered = df_cleaned[df_cleaned[target_column].isin(unique_target_categories)]\n",
    "\n",
    "        return df_filtered\n",
    "    \n",
    "generate_balanced_subsamble(\n",
    "    prepare_df(\"./datasets/weather/wrocław-EPWR.csv\"),\n",
    "    target_column='Condition',\n",
    "    target_values=\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb742931",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_df(\"./datasets/weather/wrocław-EPWR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bc73e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_sequences_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    sequence_length: int,\n",
    "    target_column: str,\n",
    "    target_keyword: str | None,\n",
    "    numerical_cols: list[str],\n",
    "    categorical_cols_encoders: dict[str, OneHotEncoder],\n",
    "    master_feature_columns: list[str],\n",
    "    nan_placeholder: str = 'missing_value',\n",
    "    step_size: int = 1\n",
    "):\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # 1. Target variable processing\n",
    "    if target_column not in df_copy.columns:\n",
    "        print(f\"Warning: Target column '{target_column}' not found in DataFrame. Skipping sequence generation for this DF.\")\n",
    "        return None\n",
    "    \n",
    "    if target_keyword is not None:\n",
    "        y_series = df_copy[target_column].astype(str).str.contains(target_keyword, case=False, na=False).astype(int)\n",
    "    else: # Handle cases where target_keyword is None (e.g., target is already binary)\n",
    "        if pd.api.types.is_bool_dtype(df_copy[target_column]):\n",
    "            y_series = df_copy[target_column].astype(int)\n",
    "        elif pd.api.types.is_numeric_dtype(df_copy[target_column]) and df_copy[target_column].dropna().isin([0, 1]).all():\n",
    "            y_series = df_copy[target_column].fillna(-1).astype(int) # FillNa if necessary, then convert. -1 for missing y? Or drop?\n",
    "            y_series = y_series[y_series != -1] # Drop rows where target was NaN\n",
    "            if y_series.empty and not df_copy[target_column].dropna().empty : # if all targets were NaN\n",
    "                 print(f\"Warning: All values in target column '{target_column}' were NaN after attempting to treat as binary. Skipping.\")\n",
    "                 return None\n",
    "        else:\n",
    "            print(f\"Error: target_keyword is None, but target_column '{target_column}' is not boolean or binary numeric. Cannot process target.\")\n",
    "            return None\n",
    "    \n",
    "    # Adjust df_copy if rows were dropped from y_series due to NaNs in target\n",
    "    if len(y_series) < len(df_copy):\n",
    "        df_copy = df_copy.loc[y_series.index]\n",
    "        if df_copy.empty:\n",
    "            return None\n",
    "\n",
    "\n",
    "    # 2. Feature Processing\n",
    "    processed_features_list = []\n",
    "\n",
    "    # Numerical features\n",
    "    present_numerical_cols = [col for col in numerical_cols if col in df_copy.columns]\n",
    "    if present_numerical_cols:\n",
    "        processed_features_list.append(df_copy[present_numerical_cols].copy())\n",
    "    else:\n",
    "        # If no numerical columns are present but some were expected, create an empty DF with original index\n",
    "        # This helps pd.concat and reindex later if only categorical features exist.\n",
    "        if numerical_cols: # only if numerical_cols list was not empty\n",
    "             processed_features_list.append(pd.DataFrame(index=df_copy.index))\n",
    "\n",
    "\n",
    "    # Categorical features\n",
    "    for cat_col_name, encoder in categorical_cols_encoders.items():\n",
    "        if cat_col_name in df_copy.columns:\n",
    "            cat_column_data = df_copy[[cat_col_name]].copy()\n",
    "            cat_column_data[cat_col_name] = cat_column_data[cat_col_name].astype(object).fillna(nan_placeholder)\n",
    "            \n",
    "            try:\n",
    "                transformed_data = encoder.transform(cat_column_data)\n",
    "                feature_names = encoder.get_feature_names_out([cat_col_name])\n",
    "                transformed_df = pd.DataFrame(transformed_data, columns=feature_names, index=df_copy.index)\n",
    "                processed_features_list.append(transformed_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error transforming categorical column {cat_col_name} with pre-fitted encoder: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: Categorical column '{cat_col_name}' (for pre-fitted encoder) not found in current DataFrame.\")\n",
    "            # We still need to account for its feature columns in the master list.\n",
    "            # Create empty columns for this encoder if it's missing, using its expected feature names.\n",
    "            try:\n",
    "                missing_encoder_feature_names = encoder.get_feature_names_out([cat_col_name])\n",
    "                missing_df = pd.DataFrame(0, index=df_copy.index, columns=missing_encoder_feature_names)\n",
    "                processed_features_list.append(missing_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not get feature names for missing {cat_col_name} to create placeholders: {e}\")\n",
    "\n",
    "\n",
    "    if not processed_features_list:\n",
    "        # This case means no numerical cols were selected AND no categorical cols processed.\n",
    "        # Reindex will handle creating an all-zero feature matrix if master_feature_columns is not empty.\n",
    "        combined_features_df = pd.DataFrame(index=df_copy.index) \n",
    "    else:\n",
    "        combined_features_df = pd.concat(processed_features_list, axis=1)\n",
    "    \n",
    "    aligned_features_df = combined_features_df.reindex(columns=master_feature_columns, fill_value=0.0)\n",
    "    X_values = aligned_features_df.astype(np.float32).values\n",
    "\n",
    "    # 3. Sequence Generation\n",
    "    X_sequences = []\n",
    "    y_labels_for_sequences = []\n",
    "    num_rows_X = len(X_values)\n",
    "    num_rows_y = len(y_series)\n",
    "\n",
    "    # Ensure X and y have aligned lengths after any potential filtering\n",
    "    if num_rows_X == 0 or num_rows_y == 0: # No data to form sequences\n",
    "        return None\n",
    "    if num_rows_X != num_rows_y: # Should not happen if df_copy.loc[y_series.index] was effective\n",
    "        print(f\"Warning: Mismatch in lengths of X ({num_rows_X}) and y ({num_rows_y}) after processing. Skipping sequences for this DF.\")\n",
    "        return None\n",
    "    \n",
    "    if num_rows_X < sequence_length:\n",
    "        return None\n",
    "\n",
    "    for i in range(0, num_rows_X - sequence_length + 1, step_size):\n",
    "        feature_seq = X_values[i : i + sequence_length]\n",
    "        X_sequences.append(feature_seq)\n",
    "        # y_series was already aligned with df_copy, which X_values is based on\n",
    "        label = y_series.iloc[i + sequence_length - 1]\n",
    "        y_labels_for_sequences.append(label)\n",
    "\n",
    "    if not X_sequences:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"X\": np.array(X_sequences, dtype=np.float32),\n",
    "        \"y\": np.array(y_labels_for_sequences, dtype=np.int32)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1bd96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(\n",
    "    train_paths: list[str],\n",
    "    test_paths: list[str],\n",
    "    prepare_df_function: callable,\n",
    "    sequence_length: int,\n",
    "    target_column: str,\n",
    "    target_keyword: str | None,\n",
    "    numerical_cols: list[str],\n",
    "    categorical_cols_encoders: dict[str, OneHotEncoder],\n",
    "    nan_placeholder: str = 'missing_value',\n",
    "    step_size: int = 1\n",
    "):\n",
    "    master_feature_columns = list(numerical_cols)\n",
    "    \n",
    "    for cat_col_name in categorical_cols_encoders:\n",
    "        encoder = categorical_cols_encoders[cat_col_name]\n",
    "        try:\n",
    "            master_feature_columns.extend(encoder.get_feature_names_out([cat_col_name]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting feature names from pre-fitted encoder for '{cat_col_name}': {e}. \"\n",
    "                  \"Ensure encoders are fitted and can produce feature names. Feature set might be incomplete.\")\n",
    "\n",
    "    if not master_feature_columns:\n",
    "        print(\"Warning: Master feature column list is empty. No features defined by numerical_cols or encoders.\")\n",
    "        empty_X_shape = (0, sequence_length, 0) if sequence_length > 0 else (0,0,0)\n",
    "        empty_result = {\"X\": np.array([]).reshape(empty_X_shape), \"y\": np.array([])}\n",
    "        return empty_result, empty_result\n",
    "    \n",
    "    print(f\"Master feature columns determined ({len(master_feature_columns)} total). Example: {master_feature_columns[:5]}...\")\n",
    "\n",
    "    def _bulk_process_paths_with_prefitted_encoders(\n",
    "        file_paths: list[str],\n",
    "        is_training_data: bool\n",
    "    ):\n",
    "        all_X_sequences_list = []\n",
    "        all_y_labels_list = []\n",
    "        \n",
    "        dataset_type = \"training\" if is_training_data else \"testing\"\n",
    "        print(f\"\\nProcessing {dataset_type} data...\")\n",
    "\n",
    "        for i, path in enumerate(file_paths):\n",
    "            print(f\"  Loading and preparing {dataset_type} file {i+1}/{len(file_paths)}: {path}\")\n",
    "            try:\n",
    "                df = prepare_df_function(path)\n",
    "            except Exception as e:\n",
    "                print(f\"    Error calling prepare_df_function for {path}: {e}. Skipping this file.\")\n",
    "                continue\n",
    "\n",
    "            if df is None or df.empty:\n",
    "                print(f\"    prepare_df_function returned None or empty DataFrame for {path}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            pypots_dict_single_df = _generate_sequences_from_df(\n",
    "                df=df,\n",
    "                sequence_length=sequence_length,\n",
    "                target_column=target_column,\n",
    "                target_keyword=target_keyword,\n",
    "                numerical_cols=numerical_cols,\n",
    "                categorical_cols_encoders=categorical_cols_encoders,\n",
    "                master_feature_columns=master_feature_columns,\n",
    "                nan_placeholder=nan_placeholder,\n",
    "                step_size=step_size\n",
    "            )\n",
    "\n",
    "            if pypots_dict_single_df and pypots_dict_single_df['X'].size > 0:\n",
    "                all_X_sequences_list.append(pypots_dict_single_df['X'])\n",
    "                all_y_labels_list.append(pypots_dict_single_df['y'])\n",
    "            else:\n",
    "                print(f\"    No sequences generated for {path}.\")\n",
    "\n",
    "        if not all_X_sequences_list:\n",
    "            print(f\"No sequences generated for any {dataset_type} files.\")\n",
    "            n_master_features = len(master_feature_columns)\n",
    "            empty_X_shape = (0, sequence_length, n_master_features) if sequence_length > 0 else (0,0,0)\n",
    "            return {\"X\": np.array([]).reshape(empty_X_shape), \"y\": np.array([])}\n",
    "\n",
    "        final_X = np.concatenate(all_X_sequences_list, axis=0)\n",
    "        final_y = np.concatenate(all_y_labels_list, axis=0)\n",
    "        return {\"X\": final_X, \"y\": final_y}\n",
    "\n",
    "    # Process training data\n",
    "    train_pypots_data = _bulk_process_paths_with_prefitted_encoders(train_paths, is_training_data=True)\n",
    "    if train_pypots_data[\"X\"].size > 0:\n",
    "        print(f\"Total training data: X shape {train_pypots_data['X'].shape}, y shape {train_pypots_data['y'].shape}\")\n",
    "\n",
    "    # Process testing data\n",
    "    test_pypots_data = _bulk_process_paths_with_prefitted_encoders(test_paths, is_training_data=False)\n",
    "    if test_pypots_data[\"X\"].size > 0:\n",
    "        print(f\"Total testing data: X shape {test_pypots_data['X'].shape}, y shape {test_pypots_data['y'].shape}\")\n",
    "        \n",
    "    return train_pypots_data, test_pypots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5c65cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 12\n",
    "STEP_SIZE = 1\n",
    "TARGET_COLUMN = 'Condition'\n",
    "FOG_KEYWORD = 'fog'\n",
    "CATEGORICAL_FEATURES = ['Wind'] # List of raw categorical column names\n",
    "NAN_PLACEHOLDER = 'missing_value' # For categoricals\n",
    "\n",
    "TRAIN_PATHS = [\n",
    "    \"./datasets/weather/wrocław-EPWR.csv\",\n",
    "    \"./datasets/weather/utrecht-IUTRECHT299.csv\",\n",
    "    \"./datasets/weather/utrecht-IUTRECHT315.csv\",\n",
    "    \"./datasets/weather/utrecht-IDEBIL13.csv\"\n",
    "]\n",
    "\n",
    "TEST_PATHS = [\n",
    "    \"./datasets/weather/utrecht-EHAM.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ca3c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master feature columns determined (7 total). Example: ['Temperature_C', 'Dew_Point_C', 'Humidity_%', 'Wind_Speed_kmh', 'Wind_Gust_kmh']...\n",
      "\n",
      "Processing training data...\n",
      "  Loading and preparing training file 1/4: ./datasets/weather/wrocław-EPWR.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196419/1984567303.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading and preparing training file 2/4: ./datasets/weather/utrecht-IUTRECHT299.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196419/1984567303.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading and preparing training file 3/4: ./datasets/weather/utrecht-IUTRECHT315.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196419/1984567303.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading and preparing training file 4/4: ./datasets/weather/utrecht-IDEBIL13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196419/1984567303.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training data: X shape (46719, 12, 7), y shape (46719,)\n",
      "\n",
      "Processing testing data...\n",
      "  Loading and preparing testing file 1/1: ./datasets/weather/utrecht-EHAM.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196419/1984567303.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total testing data: X shape (11687, 12, 7), y shape (11687,)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = prepare_datasets(\n",
    "    train_paths=TRAIN_PATHS,\n",
    "    test_paths=TEST_PATHS,\n",
    "    prepare_df_function=prepare_df,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    target_keyword=FOG_KEYWORD,\n",
    "    numerical_cols=[\"Temperature_C\",\"Dew_Point_C\",\"Humidity_%\",\"Wind_Speed_kmh\",\"Wind_Gust_kmh\",\"Pressure_hPa\",\"Precip_Rate_mm\"],\n",
    "    categorical_cols_encoders={},\n",
    "    nan_placeholder=NAN_PLACEHOLDER,\n",
    "    step_size=STEP_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "301a6f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training X shape: (46719, 12, 7)\n",
      "Final Training y shape: (46719,)\n",
      "Training y distribution: [45442  1277]\n",
      "Final Test X shape: (11687, 12, 7)\n",
      "Final Test y shape: (11687,)\n",
      "Test y distribution (if not empty): [11400   287]\n"
     ]
    }
   ],
   "source": [
    "if train_data and train_data[\"X\"].size > 0:\n",
    "    print(f\"Final Training X shape: {train_data['X'].shape}\")\n",
    "    print(f\"Final Training y shape: {train_data['y'].shape}\")\n",
    "    print(f\"Training y distribution: {np.bincount(train_data['y'])}\")\n",
    "else:\n",
    "    print(\"Training data is empty or could not be generated.\")\n",
    "\n",
    "if test_data and test_data[\"X\"].size > 0:\n",
    "    print(f\"Final Test X shape: {test_data['X'].shape}\")\n",
    "    print(f\"Final Test y shape: {test_data['y'].shape}\")\n",
    "    print(f\"Test y distribution (if not empty): {np.bincount(test_data['y']) if test_data['y'].size > 0 else 'empty'}\")\n",
    "else:\n",
    "    print(\"Test data is empty or could not be generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65e954ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample, shuffle\n",
    "from collections import Counter\n",
    "\n",
    "def balance_pypots_data(data_dict: dict, random_state: int | None=None):\n",
    "    X = data_dict['X']\n",
    "    y = data_dict['y']\n",
    "\n",
    "    class_counts = Counter(y)\n",
    "    class_labels = list(class_counts.keys())\n",
    "    if class_counts[class_labels[0]]< class_counts[class_labels[1]]:\n",
    "        minority_class_label = class_labels[0]\n",
    "        majority_class_label = class_labels[1]\n",
    "    else:\n",
    "        minority_class_label = class_labels[1]\n",
    "        majority_class_label = class_labels[0]\n",
    "    \n",
    "    minority_count = class_counts[minority_class_label]\n",
    "    majority_count = class_counts[majority_class_label]\n",
    "\n",
    "    X_minority = X[y== minority_class_label]\n",
    "    y_minority= y[y == minority_class_label]\n",
    "    X_majority = X[y== majority_class_label]\n",
    "    y_majority = y[y == majority_class_label]\n",
    "    \n",
    "    X_maj_resampled, y_maj_resampled = resample(\n",
    "        X_majority, y_majority,\n",
    "        n_samples=minority_count,\n",
    "        replace=False,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_final, y_final = shuffle(\n",
    "        np.concatenate([X_maj_resampled, X_minority], axis=0),\n",
    "        np.concatenate([y_maj_resampled, y_minority], axis=0),\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'X': X_final,\n",
    "        'y': y_final\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f78e866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training X shape: (2554, 12, 7)\n",
      "Final Training y shape: (2554,)\n",
      "Training y distribution: [1277 1277]\n",
      "Final Test X shape: (574, 12, 7)\n",
      "Final Test y shape: (574,)\n",
      "Test y distribution (if not empty): [287 287]\n"
     ]
    }
   ],
   "source": [
    "train_data_balanced = balance_pypots_data(train_data)\n",
    "test_data_balanced = balance_pypots_data(test_data)\n",
    "\n",
    "if train_data_balanced and train_data_balanced[\"X\"].size > 0:\n",
    "    print(f\"Final Training X shape: {train_data_balanced['X'].shape}\")\n",
    "    print(f\"Final Training y shape: {train_data_balanced['y'].shape}\")\n",
    "    print(f\"Training y distribution: {np.bincount(train_data_balanced['y'])}\")\n",
    "else:\n",
    "    print(\"Training data is empty or could not be generated.\")\n",
    "\n",
    "if test_data_balanced and test_data_balanced[\"X\"].size > 0:\n",
    "    print(f\"Final Test X shape: {test_data_balanced['X'].shape}\")\n",
    "    print(f\"Final Test y shape: {test_data_balanced['y'].shape}\")\n",
    "    print(f\"Test y distribution (if not empty): {np.bincount(test_data_balanced['y']) if test_data_balanced['y'].size > 0 else 'empty'}\")\n",
    "else:\n",
    "    print(\"Test data is empty or could not be generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f2a96",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f9ac0",
   "metadata": {},
   "source": [
    "## Raindrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efaf9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:42:19 [INFO]: No given device, using default device: cuda\n",
      "2025-05-12 16:42:19 [INFO]: Model files will be saved to ./runs/classify/WEATHER/raindrop/20250512_T164219\n",
      "2025-05-12 16:42:19 [INFO]: Tensorboard file will be saved to ./runs/classify/WEATHER/raindrop/20250512_T164219/tensorboard\n",
      "2025-05-12 16:42:19 [INFO]: Using customized CrossEntropy as the training loss function.\n",
      "2025-05-12 16:42:19 [INFO]: Using customized CrossEntropy as the validation metric function.\n",
      "/home/next/magisterka/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/home/next/magisterka/.venv/lib/python3.11/site-packages/pypots/nn/modules/raindrop/backbone.py:114: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  nn.init.xavier_uniform(self.R_u)  # xavier_uniform also known as glorot\n",
      "2025-05-12 16:42:21 [INFO]: Raindrop initialized with the given hyperparameters, the number of trainable parameters: 128,876\n"
     ]
    }
   ],
   "source": [
    "raindrop = Raindrop(\n",
    "    n_steps=train_data['X'].shape[1],\n",
    "    n_features=train_data['X'].shape[2],\n",
    "    n_classes=2,\n",
    "    n_layers=2,\n",
    "    d_model=train_data['X'].shape[2] * 4,\n",
    "    d_ffn=256,\n",
    "    n_heads=2,\n",
    "    dropout=0.3,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    patience=3,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=None,\n",
    "    saving_path='./runs/classify/WEATHER/raindrop',\n",
    "    model_saving_strategy='best'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68d36063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:44:01 [INFO]: Epoch 001 - training loss (CrossEntropy): 0.6093, validation CrossEntropy: 0.2561\n",
      "2025-05-12 16:44:09 [INFO]: Epoch 002 - training loss (CrossEntropy): 0.4015, validation CrossEntropy: 0.2425\n",
      "2025-05-12 16:44:17 [INFO]: Epoch 003 - training loss (CrossEntropy): 0.3692, validation CrossEntropy: 0.3499\n",
      "2025-05-12 16:44:27 [INFO]: Epoch 004 - training loss (CrossEntropy): 0.3623, validation CrossEntropy: 0.2336\n",
      "2025-05-12 16:44:39 [INFO]: Epoch 005 - training loss (CrossEntropy): 0.3507, validation CrossEntropy: 0.2524\n",
      "2025-05-12 16:44:48 [INFO]: Epoch 006 - training loss (CrossEntropy): 0.3491, validation CrossEntropy: 0.2743\n",
      "2025-05-12 16:44:58 [INFO]: Epoch 007 - training loss (CrossEntropy): 0.3360, validation CrossEntropy: 0.4760\n",
      "2025-05-12 16:44:58 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-12 16:44:58 [INFO]: Finished training. The best model is from epoch#4.\n",
      "2025-05-12 16:44:58 [INFO]: Saved the model to ./runs/classify/WEATHER/raindrop/20250512_T164219/Raindrop.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.9163763066202091, \n",
      "PR_AUC: 0.9391142881942293,\n",
      "F1: 0.9154929577464789,\n",
      "Precision: 0.9252669039145908,\n",
      "Recall: 0.9059233449477352,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raindrop.fit(train_set=train_data_balanced, val_set=test_data_balanced)\n",
    "results = raindrop.predict(test_data_balanced)\n",
    "prediction = results['classification']\n",
    "metrics = calc_binary_classification_metrics(prediction, test_data_balanced['y'])\n",
    "print(\"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf20327",
   "metadata": {},
   "source": [
    "## BRITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0847e160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:12:58 [INFO]: No given device, using default device: cuda\n",
      "2025-05-12 15:12:58 [INFO]: Model files will be saved to ./runs/classify/WEATHER/brits/20250512_T151258\n",
      "2025-05-12 15:12:58 [INFO]: Tensorboard file will be saved to ./runs/classify/WEATHER/brits/20250512_T151258/tensorboard\n",
      "2025-05-12 15:12:58 [INFO]: Using customized CrossEntropy as the training loss function.\n",
      "2025-05-12 15:12:58 [INFO]: Using customized CrossEntropy as the validation metric function.\n",
      "2025-05-12 15:13:00 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 566,212\n"
     ]
    }
   ],
   "source": [
    "brits = BRITS(\n",
    "    n_steps=train_data['X'].shape[1],\n",
    "    n_features=train_data['X'].shape[2],\n",
    "    n_classes=2,\n",
    "    rnn_hidden_size=256,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    patience=3,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=None,\n",
    "    saving_path='./runs/classify/WEATHER/brits',\n",
    "    model_saving_strategy='best'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "735b1014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:22:59 [INFO]: Epoch 001 - training loss (CrossEntropy): 286.7109, validation CrossEntropy: 0.1258\n",
      "2025-05-12 15:29:50 [INFO]: Epoch 002 - training loss (CrossEntropy): 229.2128, validation CrossEntropy: 0.1219\n",
      "2025-05-12 15:36:04 [INFO]: Epoch 003 - training loss (CrossEntropy): 183.3828, validation CrossEntropy: 0.1189\n",
      "2025-05-12 15:42:13 [INFO]: Epoch 004 - training loss (CrossEntropy): 138.8626, validation CrossEntropy: 0.1197\n",
      "2025-05-12 15:48:23 [INFO]: Epoch 005 - training loss (CrossEntropy): 97.0802, validation CrossEntropy: 0.1215\n",
      "2025-05-12 15:54:29 [INFO]: Epoch 006 - training loss (CrossEntropy): 68.2444, validation CrossEntropy: 0.1156\n",
      "2025-05-12 16:00:41 [INFO]: Epoch 007 - training loss (CrossEntropy): 53.2646, validation CrossEntropy: 0.1157\n",
      "2025-05-12 16:06:59 [INFO]: Epoch 008 - training loss (CrossEntropy): 39.8244, validation CrossEntropy: 0.1166\n",
      "2025-05-12 16:13:10 [INFO]: Epoch 009 - training loss (CrossEntropy): 34.1167, validation CrossEntropy: 0.1128\n",
      "2025-05-12 16:18:45 [INFO]: Epoch 010 - training loss (CrossEntropy): 33.0141, validation CrossEntropy: 0.1052\n",
      "2025-05-12 16:18:45 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-05-12 16:18:45 [INFO]: Saved the model to ./runs/classify/WEATHER/brits/20250512_T151258/BRITS.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.5, \n",
      "PR_AUC: 0.5122786001540173,\n",
      "F1: 0.0,\n",
      "Precision: 0.0,\n",
      "Recall: 0.0,\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/next/magisterka/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "brits.fit(train_set=train_data, val_set=test_data)\n",
    "results = brits.predict(test_data)\n",
    "prediction = results['classification']\n",
    "metrics = calc_binary_classification_metrics(prediction, test_data['y'])\n",
    "print(\"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4f4322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:45:13 [INFO]: No given device, using default device: cuda\n",
      "2025-05-12 16:45:13 [INFO]: Model files will be saved to ./runs/classify/WEATHER/brits/20250512_T164513\n",
      "2025-05-12 16:45:13 [INFO]: Tensorboard file will be saved to ./runs/classify/WEATHER/brits/20250512_T164513/tensorboard\n",
      "2025-05-12 16:45:13 [INFO]: Using customized CrossEntropy as the training loss function.\n",
      "2025-05-12 16:45:13 [INFO]: Using customized CrossEntropy as the validation metric function.\n",
      "2025-05-12 16:45:13 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 566,212\n"
     ]
    }
   ],
   "source": [
    "brits2 = BRITS(\n",
    "    n_steps=train_data_balanced['X'].shape[1],\n",
    "    n_features=train_data_balanced['X'].shape[2],\n",
    "    n_classes=2,\n",
    "    rnn_hidden_size=256,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    patience=3,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=None,\n",
    "    saving_path='./runs/classify/WEATHER/brits',\n",
    "    model_saving_strategy='best'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94723aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:45:41 [INFO]: Epoch 001 - training loss (CrossEntropy): 385.1241, validation CrossEntropy: 0.6301\n",
      "2025-05-12 16:46:01 [INFO]: Epoch 002 - training loss (CrossEntropy): 332.2639, validation CrossEntropy: 0.5668\n",
      "2025-05-12 16:46:25 [INFO]: Epoch 003 - training loss (CrossEntropy): 308.5136, validation CrossEntropy: 0.5203\n",
      "2025-05-12 16:46:49 [INFO]: Epoch 004 - training loss (CrossEntropy): 300.3549, validation CrossEntropy: 0.5246\n",
      "2025-05-12 16:47:10 [INFO]: Epoch 005 - training loss (CrossEntropy): 295.8680, validation CrossEntropy: 0.5210\n",
      "2025-05-12 16:47:32 [INFO]: Epoch 006 - training loss (CrossEntropy): 292.3525, validation CrossEntropy: 0.4652\n",
      "2025-05-12 16:47:53 [INFO]: Epoch 007 - training loss (CrossEntropy): 288.4602, validation CrossEntropy: 0.4832\n",
      "2025-05-12 16:48:16 [INFO]: Epoch 008 - training loss (CrossEntropy): 284.6853, validation CrossEntropy: 0.4643\n",
      "2025-05-12 16:48:39 [INFO]: Epoch 009 - training loss (CrossEntropy): 281.1611, validation CrossEntropy: 0.4314\n",
      "2025-05-12 16:48:59 [INFO]: Epoch 010 - training loss (CrossEntropy): 277.7401, validation CrossEntropy: 0.4308\n",
      "2025-05-12 16:48:59 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-05-12 16:48:59 [INFO]: Saved the model to ./runs/classify/WEATHER/brits/20250512_T164513/BRITS.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.9268292682926829, \n",
      "PR_AUC: 0.9417832962371668,\n",
      "F1: 0.9283276450511946,\n",
      "Precision: 0.9096989966555183,\n",
      "Recall: 0.9477351916376306,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brits2.fit(train_set=train_data_balanced, val_set=test_data_balanced)\n",
    "results = brits2.predict(test_data_balanced)\n",
    "prediction = results['classification']\n",
    "metrics = calc_binary_classification_metrics(prediction, test_data_balanced['y'])\n",
    "print(\"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    "    f'Accuracy: {metrics[\"accuracy\"]},\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3130f343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.926829268292683\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {metrics[\"accuracy\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
