{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3efc4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pypots.utils.random import set_random_seed\n",
    "from pypots.optim import Adam\n",
    "from pypots.classification import Raindrop, BRITS, GRUD\n",
    "from pypots.nn.functional import calc_binary_classification_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b25e2",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7593c",
   "metadata": {},
   "source": [
    "## üìç Station Information\n",
    "\n",
    "| Field        | Description                                      |\n",
    "|--------------|--------------------------------------------------|\n",
    "| `STN`        | Station number                                   |\n",
    "| `LON(east)`  | Longitude (degrees east)                         |\n",
    "| `LAT(north)` | Latitude (degrees north)                         |\n",
    "| `ALT(m)`     | Altitude (in meters)                             |\n",
    "| `NAME`       | Station name                                     |\n",
    "\n",
    "## üìÖ Time Information\n",
    "\n",
    "| Field       | Description |\n",
    "|-------------|-------------|\n",
    "| `YYYYMMDD`  | Date (YYYY = year; MM = month; DD = day) |\n",
    "| `HH`        | Hour (HH = hour; UT time. E.g., 12 UT = 13 MET, 14 MEZT. Hour block 05 = 04:00‚Äì05:00 UT) |\n",
    "\n",
    "## üå¨Ô∏è Wind Data\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| `DD`  | Wind direction (in degrees), averaged over the last 10 minutes of the previous hour<br>(360 = north, 90 = east, 180 = south, 270 = west, 0 = calm, 990 = variable) |\n",
    "| `FH`  | Hourly mean wind speed (in 0.1 m/s) |\n",
    "| `FF`  | Mean wind speed (in 0.1 m/s) during last 10 minutes of the previous hour |\n",
    "| `FX`  | Maximum wind gust (in 0.1 m/s) during the hour block |\n",
    "\n",
    "## üå°Ô∏è Temperature & Humidity\n",
    "\n",
    "| Field   | Description |\n",
    "|---------|-------------|\n",
    "| `T`     | Temperature (in 0.1‚ÄØ¬∞C) at 1.50‚ÄØm height at observation time |\n",
    "| `T10N`  | Minimum temperature (in 0.1‚ÄØ¬∞C) at 10‚ÄØcm height in the past 6 hours |\n",
    "| `TD`    | Dew point temperature (in 0.1‚ÄØ¬∞C) at 1.50‚ÄØm at observation time |\n",
    "| `U`     | Relative humidity (%) at 1.50‚ÄØm height |\n",
    "\n",
    "## üåû Radiation & Sunshine\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| `SQ`  | Sunshine duration (in 0.1 hours) per hour block, calculated from global radiation<br>(-1 = less than 0.05‚ÄØh) |\n",
    "| `Q`   | Global radiation (in J/cm¬≤) per hour block |\n",
    "\n",
    "## üåßÔ∏è Precipitation\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| `DR`  | Duration of precipitation (in 0.1 hours) per hour block |\n",
    "| `RH`  | Hourly precipitation amount (in 0.1 mm)<br>(-1 = less than 0.05 mm) |\n",
    "\n",
    "## üå¨Ô∏è Atmospheric Pressure\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| `P`   | Air pressure (in 0.1 hPa), reduced to sea level, at observation time |\n",
    "\n",
    "## üëÅÔ∏è Visibility & Clouds\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| `VV`  | Horizontal visibility at observation time<br>(0 = <100‚ÄØm, 1 = 100‚Äì200‚ÄØm, ..., 89 = >70‚ÄØkm) |\n",
    "| `N`   | Cloud cover (in octants, 0‚Äì8); 9 = sky invisible |\n",
    "\n",
    "## üå¶Ô∏è Weather Conditions\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| `WW`  | Present weather code (00‚Äì99); observed visually or automatically |\n",
    "| `IX`  | Weather code observation type:<br>1 = manned/visual; 2/3 = manned/omitted;<br>4 = automatic/visual; 5/6 = automatic/omitted; 7 = automatic sensor-based |\n",
    "\n",
    "## üå´Ô∏è Weather Phenomena Indicators\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| `M`   | Fog: 0 = none; 1 = occurred during the previous hour and/or observation |\n",
    "| `R`   | Rain: 0 = none; 1 = occurred during the previous hour and/or observation |\n",
    "| `S`   | Snow: 0 = none; 1 = occurred during the previous hour and/or observation |\n",
    "| `O`   | Thunder: 0 = none; 1 = occurred during the previous hour and/or observation |\n",
    "| `Y`   | Ice formation: 0 = none; 1 = occurred during the previous hour and/or observation |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d560178e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STN</th>\n",
       "      <th>YYYYMMDD</th>\n",
       "      <th>HH</th>\n",
       "      <th>DD</th>\n",
       "      <th>FH</th>\n",
       "      <th>FF</th>\n",
       "      <th>FX</th>\n",
       "      <th>T</th>\n",
       "      <th>T10N</th>\n",
       "      <th>TD</th>\n",
       "      <th>...</th>\n",
       "      <th>VV</th>\n",
       "      <th>N</th>\n",
       "      <th>U</th>\n",
       "      <th>WW</th>\n",
       "      <th>IX</th>\n",
       "      <th>M</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>O</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>215</td>\n",
       "      <td>20150101</td>\n",
       "      <td>0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00</th>\n",
       "      <td>215</td>\n",
       "      <td>20150101</td>\n",
       "      <td>1</td>\n",
       "      <td>220.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00</th>\n",
       "      <td>215</td>\n",
       "      <td>20150101</td>\n",
       "      <td>2</td>\n",
       "      <td>200.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 03:00:00</th>\n",
       "      <td>215</td>\n",
       "      <td>20150101</td>\n",
       "      <td>3</td>\n",
       "      <td>210.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 04:00:00</th>\n",
       "      <td>215</td>\n",
       "      <td>20150101</td>\n",
       "      <td>4</td>\n",
       "      <td>190.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     STN  YYYYMMDD  HH     DD    FH    FF    FX   T  T10N  TD  \\\n",
       "Timestamp                                                                       \n",
       "2015-01-01 00:00:00  215  20150101   0  210.0  50.0  50.0  70.0  27   NaN   8   \n",
       "2015-01-01 01:00:00  215  20150101   1  220.0  50.0  50.0  70.0  26   NaN   4   \n",
       "2015-01-01 02:00:00  215  20150101   2  200.0  50.0  40.0  80.0  23   NaN   2   \n",
       "2015-01-01 03:00:00  215  20150101   3  210.0  40.0  40.0  70.0  21   NaN   1   \n",
       "2015-01-01 04:00:00  215  20150101   4  190.0  50.0  50.0  80.0  19   NaN   2   \n",
       "\n",
       "                     ...    VV    N   U    WW  IX    M    R    S    O    Y  \n",
       "Timestamp            ...                                                    \n",
       "2015-01-01 00:00:00  ...  42.0  0.0  87  10.0   7  0.0  0.0  0.0  0.0  0.0  \n",
       "2015-01-01 01:00:00  ...  57.0  0.0  85  10.0   7  0.0  0.0  0.0  0.0  0.0  \n",
       "2015-01-01 02:00:00  ...  60.0  0.0  86   NaN   5  0.0  0.0  0.0  0.0  0.0  \n",
       "2015-01-01 03:00:00  ...  60.0  0.0  87   NaN   5  0.0  0.0  0.0  0.0  0.0  \n",
       "2015-01-01 04:00:00  ...  60.0  1.0  88   NaN   5  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_df(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        header_line_index = -1\n",
    "        column_names = []\n",
    "        data_lines_start_index = -1\n",
    "\n",
    "        # Find the header and its index more efficiently\n",
    "        with open(path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if line.strip().startswith('# STN,YYYYMMDD,'):\n",
    "                    header_line_index = i\n",
    "                    column_names = [col.strip() for col in line.strip().lstrip('#').split(',')]\n",
    "                    data_lines_start_index = header_line_index + 1\n",
    "                    break\n",
    "        \n",
    "        if header_line_index == -1:\n",
    "            raise ValueError(\"Header line not found.\")\n",
    "\n",
    "        # Use pandas.read_csv directly with skiprows and comment character\n",
    "        # This avoids reading the whole file into a list first for data lines\n",
    "        # and then joining them back.\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            names=column_names,\n",
    "            skiprows=data_lines_start_index,\n",
    "            comment='#',  # Lines starting with '#' will be ignored as comments\n",
    "            skipinitialspace=True,\n",
    "            na_values=['       ', '     '] # Add other common missing value representations if needed\n",
    "        )\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"No data found after the header or all data was commented out.\")\n",
    "\n",
    "        # Convert 'HH' to string and zfill, then create 'Timestamp'\n",
    "        # It's crucial to handle potential NaN values in 'YYYYMMDD' or 'HH'\n",
    "        # if they are not guaranteed to be present or valid in all rows.\n",
    "        df['HH'] = df['HH'].astype(int) - 1\n",
    "        df['HH'] = df['HH'].astype(str).str.zfill(2)\n",
    "        df['Timestamp'] = pd.to_datetime(df['YYYYMMDD'].astype(str) + df['HH'].astype(str), format=\"%Y%m%d%H\", errors='coerce')\n",
    "        \n",
    "        df.set_index('Timestamp', inplace=True)\n",
    "        \n",
    "        # Columns to drop\n",
    "        # cols_to_drop = ['YYYYMMDD', 'HH']\n",
    "        # df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "        # Convert remaining columns to numeric, efficiently\n",
    "        # Identify numeric columns once and convert\n",
    "        # Exclude already processed or known non-numeric columns if necessary\n",
    "        for col in df.columns:\n",
    "            # This check is slightly redundant if YYYYMMDD and HH are already dropped,\n",
    "            # but good for safety if they weren't or if other non-numeric columns exist.\n",
    "            if df[col].dtype == 'object': # Only attempt conversion if the column is of object type\n",
    "                try:\n",
    "                    df[col] = pd.to_numeric(df[col], downcast='signed')\n",
    "                except ValueError:\n",
    "                    # Handle or log cases where a column expected to be numeric isn't\n",
    "                    # For now, we'll coerce, which turns unparseable into NaT/NaN\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce', downcast='signed')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{path}' was not found.\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "df = prepare_df(\"./datasets/knmi_station_data/215.txt\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a83649ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VV\n",
       "75.0    4402\n",
       "70.0    4249\n",
       "65.0    2454\n",
       "80.0    2269\n",
       "63.0    1196\n",
       "        ... \n",
       "15.0      51\n",
       "8.0       49\n",
       "14.0      48\n",
       "9.0       42\n",
       "10.0      40\n",
       "Name: count, Length: 79, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Horizontal visibility at observation time\n",
    "# (0 = <100‚ÄØm, 1 = 100‚Äì200‚ÄØm, ..., 89 = >70‚ÄØkm)\n",
    "df['VV'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03c530fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STN</th>\n",
       "      <th>YYYYMMDD</th>\n",
       "      <th>HH</th>\n",
       "      <th>DD</th>\n",
       "      <th>FH</th>\n",
       "      <th>FF</th>\n",
       "      <th>FX</th>\n",
       "      <th>T</th>\n",
       "      <th>T10N</th>\n",
       "      <th>TD</th>\n",
       "      <th>...</th>\n",
       "      <th>VV</th>\n",
       "      <th>N</th>\n",
       "      <th>U</th>\n",
       "      <th>WW</th>\n",
       "      <th>IX</th>\n",
       "      <th>M</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>O</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-23 04:00:00</th>\n",
       "      <td>215</td>\n",
       "      <td>20150123</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-60</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-23 06:00:00</th>\n",
       "      <td>215</td>\n",
       "      <td>20150123</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-57</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-17 01:00:00</th>\n",
       "      <td>215</td>\n",
       "      <td>20150217</td>\n",
       "      <td>1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>98</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-17 05:00:00</th>\n",
       "      <td>215</td>\n",
       "      <td>20150217</td>\n",
       "      <td>5</td>\n",
       "      <td>210.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>42</td>\n",
       "      <td>26.0</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>97</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-07 07:00:00</th>\n",
       "      <td>215</td>\n",
       "      <td>20150407</td>\n",
       "      <td>7</td>\n",
       "      <td>300.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>98</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     STN  YYYYMMDD  HH     DD    FH    FF    FX   T  T10N  TD  \\\n",
       "Timestamp                                                                       \n",
       "2015-01-23 04:00:00  215  20150123   4    0.0   0.0   0.0  10.0 -53   NaN -60   \n",
       "2015-01-23 06:00:00  215  20150123   6    0.0  10.0   0.0  10.0 -52   NaN -57   \n",
       "2015-02-17 01:00:00  215  20150217   1  240.0  20.0  20.0  50.0  35   NaN  32   \n",
       "2015-02-17 05:00:00  215  20150217   5  210.0  10.0  10.0  20.0  42  26.0  37   \n",
       "2015-04-07 07:00:00  215  20150407   7  300.0  30.0  30.0  40.0  56   NaN  53   \n",
       "\n",
       "                     ...    VV    N   U    WW  IX    M    R    S    O    Y  \n",
       "Timestamp            ...                                                    \n",
       "2015-01-23 04:00:00  ...  12.0  0.0  94  10.0   7  0.0  0.0  0.0  0.0  0.0  \n",
       "2015-01-23 06:00:00  ...  12.0  0.0  96  10.0   7  0.0  0.0  0.0  0.0  0.0  \n",
       "2015-02-17 01:00:00  ...  12.0  8.0  98  20.0   7  1.0  0.0  0.0  0.0  0.0  \n",
       "2015-02-17 05:00:00  ...  12.0  8.0  97  10.0   7  0.0  0.0  0.0  0.0  0.0  \n",
       "2015-04-07 07:00:00  ...  12.0  8.0  98  20.0   7  1.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['VV'] == 12].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ee0d186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STN: 1\n",
      "DD: 38\n",
      "FH: 21\n",
      "FF: 22\n",
      "FX: 33\n",
      "T: 353\n",
      "T10N: 295\n",
      "TD: 307\n",
      "SQ: 12\n",
      "Q: 323\n",
      "DR: 11\n",
      "RH: 67\n",
      "P: 711\n",
      "VV: 79\n",
      "N: 10\n",
      "U: 76\n",
      "WW: 41\n",
      "IX: 3\n",
      "M: 2\n",
      "R: 2\n",
      "S: 2\n",
      "O: 2\n",
      "Y: 2\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    print(f\"{col}: {len(df[col].value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e67b7b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DD\n",
       "200.0    1768\n",
       "230.0    1692\n",
       "240.0    1682\n",
       "210.0    1541\n",
       "250.0    1530\n",
       "220.0    1324\n",
       "260.0    1264\n",
       "190.0    1253\n",
       "270.0    1110\n",
       "180.0    1101\n",
       "60.0     1092\n",
       "70.0     1090\n",
       "340.0    1056\n",
       "280.0    1055\n",
       "330.0    1047\n",
       "320.0    1040\n",
       "80.0     1036\n",
       "350.0    1030\n",
       "170.0     953\n",
       "110.0     943\n",
       "290.0     910\n",
       "50.0      898\n",
       "310.0     891\n",
       "130.0     785\n",
       "360.0     775\n",
       "140.0     758\n",
       "100.0     754\n",
       "300.0     753\n",
       "120.0     747\n",
       "10.0      739\n",
       "40.0      721\n",
       "30.0      714\n",
       "0.0       625\n",
       "90.0      598\n",
       "20.0      591\n",
       "160.0     580\n",
       "150.0     532\n",
       "990.0     241\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"DD\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cfc8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_na_stats(df, col):\n",
    "    na = df[col].isna().sum()\n",
    "    with_value = df[col].fillna(0).astype(bool).sum()\n",
    "    print(f\"{col}: {na}, {with_value}, {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "833ca335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 203, 1839, 37248\n",
      "T10N: 31040, 6172, 37248\n"
     ]
    }
   ],
   "source": [
    "get_na_stats(df, 'M')\n",
    "get_na_stats(df, 'T10N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bc73e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # Ensure this is imported\n",
    "\n",
    "# Helper function for core processing and sequence creation\n",
    "def _process_and_create_sequences_internal(\n",
    "        df_segment: pd.DataFrame,\n",
    "        target_column_name: str,\n",
    "        numerical_features_cols: list[str],\n",
    "        categorical_features_cols: dict[str, OneHotEncoder],\n",
    "        prev_time_steps: int,\n",
    ") -> dict[str, np.ndarray]:\n",
    "\n",
    "    # This df_segment is assumed to have target_column_name NaNs already handled if desired.\n",
    "    \n",
    "    processed_feature_dfs_list = []\n",
    "\n",
    "    # 1. Process Numerical Features\n",
    "    if numerical_features_cols:\n",
    "        valid_numerical_cols = [col for col in numerical_features_cols if col in df_segment.columns]\n",
    "        if valid_numerical_cols: # Only proceed if there are valid numerical columns to select\n",
    "            numerical_df = df_segment[valid_numerical_cols].copy()\n",
    "            processed_feature_dfs_list.append(numerical_df)\n",
    "        elif not valid_numerical_cols and numerical_features_cols: # Specified but none found\n",
    "            print(f\"Warning (internal): None of the specified numerical features found in the current data segment. Numerical features count: {len(numerical_features_cols)}\")\n",
    "\n",
    "\n",
    "    # 2. Process Categorical Features\n",
    "    for col_name, encoder in categorical_features_cols.items():\n",
    "        if col_name not in df_segment.columns:\n",
    "            # This happens if df_segment is empty or the column was dropped.\n",
    "            # The impact on num_actual_features will be handled later.\n",
    "            continue\n",
    "\n",
    "        column_to_encode = df_segment[[col_name]]\n",
    "        encoded_data_sparse = np.array([]) # Default empty\n",
    "        \n",
    "        if column_to_encode.empty: # Encoder might not handle empty DataFrame input well for transform if it expects rows\n",
    "             try: # Get feature names to create an empty DataFrame with correct OHE columns\n",
    "                ohe_feature_names = encoder.get_feature_names_out([col_name])\n",
    "             except AttributeError:\n",
    "                ohe_feature_names = encoder.get_feature_names([col_name])\n",
    "             except Exception:\n",
    "                ohe_feature_names = [f\"{col_name}_cat{i}\" for i in range(len(encoder.categories_[0]))] if hasattr(encoder, 'categories_') else [f\"{col_name}_unknown_cat\"]\n",
    "             \n",
    "             # Create an empty DataFrame (0 rows) with the OHE column names\n",
    "             ohe_df = pd.DataFrame(columns=ohe_feature_names, index=df_segment.index, dtype=float)\n",
    "             processed_feature_dfs_list.append(ohe_df)\n",
    "             continue # Go to next categorical column\n",
    "\n",
    "        # If not empty, proceed with transform\n",
    "        encoded_data_sparse = encoder.transform(column_to_encode)\n",
    "        \n",
    "        try:\n",
    "            ohe_feature_names = encoder.get_feature_names_out([col_name])\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                ohe_feature_names = encoder.get_feature_names([col_name])\n",
    "            except TypeError:\n",
    "                ohe_feature_names = [f\"{col_name}_{category}\" for category in encoder.categories_[0]]\n",
    "            except Exception as e_fn: # Broad exception for get_feature_names issues\n",
    "                print(f\"Warning (internal): Could not reliably get OHE feature names for '{col_name}'. Using generic names. Error: {e_fn}\")\n",
    "                num_output_features = encoded_data_sparse.shape[1]\n",
    "                ohe_feature_names = [f\"{col_name}_ohe_{i}\" for i in range(num_output_features)]\n",
    "\n",
    "        if hasattr(encoded_data_sparse, \"toarray\"):\n",
    "            encoded_data_dense = encoded_data_sparse.toarray()\n",
    "        else:\n",
    "            encoded_data_dense = encoded_data_sparse\n",
    "        \n",
    "        if encoded_data_dense.shape[0] > 0 : # Ensure data was actually produced\n",
    "            ohe_df = pd.DataFrame(encoded_data_dense, columns=ohe_feature_names, index=df_segment.index)\n",
    "            processed_feature_dfs_list.append(ohe_df)\n",
    "        elif df_segment.shape[0] > 0 : # Input segment had rows, but OHE produced no rows (should not happen with sklearn)\n",
    "             print(f\"Warning (internal): OHE for '{col_name}' produced 0 rows from a non-empty segment. Check encoder.\")\n",
    "\n",
    "\n",
    "    # 3. Combine all processed feature DataFrames\n",
    "    num_actual_features = 0\n",
    "    if not processed_feature_dfs_list:\n",
    "        final_features_df = pd.DataFrame(index=df_segment.index) # 0 columns\n",
    "        if numerical_features_cols or categorical_features_cols:\n",
    "             print(\"Warning (internal): No features were processed into final_features_df despite specification. X will have 0 features for this segment.\")\n",
    "    else:\n",
    "        final_features_df = pd.concat(processed_feature_dfs_list, axis=1)\n",
    "    num_actual_features = final_features_df.shape[1]\n",
    "\n",
    "\n",
    "    # --- Sequence Creation ---\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    num_rows_in_features = len(final_features_df)\n",
    "\n",
    "    if prev_time_steps >= num_rows_in_features :\n",
    "        return {'X': np.array([]).reshape(0, prev_time_steps, num_actual_features), 'y': np.array([])}\n",
    "\n",
    "    y_series_segment = df_segment[target_column_name]\n",
    "\n",
    "    for i in range(prev_time_steps, num_rows_in_features):\n",
    "        feature_sequence = final_features_df.iloc[i - prev_time_steps : i].values\n",
    "        X_list.append(feature_sequence)\n",
    "        target_value = y_series_segment.iloc[i]\n",
    "        y_list.append(target_value)\n",
    "\n",
    "    if not X_list:\n",
    "        return {'X': np.array([]).reshape(0, prev_time_steps, num_actual_features), 'y': np.array([])}\n",
    "        \n",
    "    X_np = np.array(X_list)\n",
    "    y_np = np.array(y_list)\n",
    "    \n",
    "    return {'X': X_np, 'y': y_np}\n",
    "\n",
    "# Helper for balancing data (undersampling)\n",
    "def _balance_data_helper(X_in: np.ndarray, y_in: np.ndarray, random_seed: int | None = None) -> tuple[np.ndarray, np.ndarray]:\n",
    "    if X_in.shape[0] == 0: # No data to balance\n",
    "        return X_in, y_in\n",
    "\n",
    "    unique_classes, counts = np.unique(y_in, return_counts=True)\n",
    "    \n",
    "    if len(unique_classes) <= 1: # Already balanced or only one class\n",
    "        return X_in, y_in\n",
    "\n",
    "    min_count = np.min(counts)\n",
    "    \n",
    "    balanced_indices_list = []\n",
    "    rng = np.random.RandomState(random_seed) # For reproducible undersampling if seed is provided\n",
    "\n",
    "    for cls_val in unique_classes:\n",
    "        cls_indices = np.where(y_in == cls_val)[0]\n",
    "        if len(cls_indices) > min_count:\n",
    "            chosen_indices = rng.choice(cls_indices, size=min_count, replace=False)\n",
    "        else:\n",
    "            chosen_indices = cls_indices # Take all if it's already the min count or less\n",
    "        balanced_indices_list.extend(chosen_indices)\n",
    "    \n",
    "    # Shuffle the combined indices from different classes\n",
    "    shuffled_balanced_indices = rng.permutation(balanced_indices_list)\n",
    "    \n",
    "    return X_in[shuffled_balanced_indices], y_in[shuffled_balanced_indices]\n",
    "\n",
    "\n",
    "# Main Function\n",
    "def create_sequences_for_classification(\n",
    "        df: pd.DataFrame,\n",
    "        target_column_name: str,\n",
    "        numerical_features_cols: list[str],\n",
    "        categorical_features_cols: dict[str, OneHotEncoder], # {col_name: fitted_encoder}\n",
    "        prev_time_steps: int = 8,\n",
    "        split_date: str | pd.Timestamp | None = None,\n",
    "        balance_data: bool = False,\n",
    "        balance_random_seed: int | None = None, # Seed for balancing reproducibility\n",
    ") -> dict[str, np.ndarray] | tuple[dict[str, np.ndarray], dict[str, np.ndarray]]:\n",
    "\n",
    "    if target_column_name not in df.columns: # Check on original df\n",
    "        raise ValueError(f\"Target column '{target_column_name}' not found in DataFrame.\")\n",
    "    \n",
    "    df_features = df.copy() # Use the name from your stub\n",
    "    # --- User requested modification: remove rows where target is NaN ---\n",
    "    df_features = df_features[~df_features[target_column_name].isna()]\n",
    "    # --- End of modification ---\n",
    "\n",
    "    if df_features.empty:\n",
    "        print(f\"DataFrame is empty after removing NaNs in target column '{target_column_name}'.\")\n",
    "        # The _process_and_create_sequences_internal can handle an empty df_features\n",
    "        # and return correctly shaped empty arrays.\n",
    "        empty_sequences = _process_and_create_sequences_internal(\n",
    "            df_features, target_column_name, numerical_features_cols,\n",
    "            categorical_features_cols, prev_time_steps\n",
    "        )\n",
    "        if split_date:\n",
    "            return empty_sequences, empty_sequences.copy() # Return two empty dicts\n",
    "        else:\n",
    "            return empty_sequences\n",
    "\n",
    "\n",
    "    # Basic Validations for other parameters\n",
    "    if not isinstance(prev_time_steps, int) or prev_time_steps <= 0:\n",
    "        raise ValueError(\"'prev_time_steps' must be a positive integer.\")\n",
    "    for col in numerical_features_cols:\n",
    "        if col not in df_features.columns: # Check on df_features as it's the one being processed\n",
    "            print(f\"Warning: Numerical feature column '{col}' not found in (cleaned) DataFrame. It will be ignored if missing.\")\n",
    "    for col in categorical_features_cols.keys():\n",
    "        if col not in df_features.columns:\n",
    "            print(f\"Warning: Categorical feature column '{col}' not found in (cleaned) DataFrame. It will be ignored if missing.\")\n",
    "    \n",
    "    if split_date:\n",
    "        if not isinstance(df_features.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"DataFrame index must be a DatetimeIndex for date-based splitting.\")\n",
    "        \n",
    "        split_date_ts = pd.to_datetime(split_date) # Converts str or pd.Timestamp\n",
    "\n",
    "        train_df = df_features[df_features.index <= split_date_ts].copy() # Use .copy() to avoid SettingWithCopyWarning later\n",
    "        test_df = df_features[df_features.index > split_date_ts].copy()\n",
    "\n",
    "        print(f\"Original df_features shape: {df_features.shape}\")\n",
    "        print(f\"Train data shape (after target NaN drop, before sequence creation): {train_df.shape}\")\n",
    "        print(f\"Test data shape (after target NaN drop, before sequence creation): {test_df.shape}\")\n",
    "\n",
    "        if train_df.empty:\n",
    "            print(\"Warning: Train DataFrame is empty after split.\")\n",
    "        if test_df.empty:\n",
    "            print(\"Warning: Test DataFrame is empty after split.\")\n",
    "\n",
    "        train_sequences = _process_and_create_sequences_internal(\n",
    "            train_df, target_column_name, numerical_features_cols,\n",
    "            categorical_features_cols, prev_time_steps\n",
    "        )\n",
    "        test_sequences = _process_and_create_sequences_internal(\n",
    "            test_df, target_column_name, numerical_features_cols,\n",
    "            categorical_features_cols, prev_time_steps\n",
    "        )\n",
    "\n",
    "        if balance_data and train_sequences['X'].shape[0] > 0:\n",
    "            print(f\"Balancing training data... Original counts: {dict(zip(*np.unique(train_sequences['y'], return_counts=True)))}\")\n",
    "            train_sequences['X'], train_sequences['y'] = _balance_data_helper(\n",
    "                train_sequences['X'], train_sequences['y'], random_seed=balance_random_seed\n",
    "            )\n",
    "            print(f\"Balanced training data shapes: X={train_sequences['X'].shape}, y={train_sequences['y'].shape}. New counts: {dict(zip(*np.unique(train_sequences['y'], return_counts=True)))}\")\n",
    "        if balance_data and test_sequences['X'].shape[0] > 0:\n",
    "            test_sequences['X'], test_sequences['y'] = _balance_data_helper(\n",
    "                test_sequences['X'], test_sequences['y'], random_seed=balance_random_seed\n",
    "            )\n",
    "        return train_sequences, test_sequences\n",
    "    else:\n",
    "        # No split, process the whole df_features\n",
    "        all_sequences = _process_and_create_sequences_internal(\n",
    "            df_features, target_column_name, numerical_features_cols,\n",
    "            categorical_features_cols, prev_time_steps\n",
    "        )\n",
    "        \n",
    "        if balance_data and all_sequences['X'].shape[0] > 0:\n",
    "            print(f\"Balancing all data... Original counts: {dict(zip(*np.unique(all_sequences['y'], return_counts=True)))}\")\n",
    "            all_sequences['X'], all_sequences['y'] = _balance_data_helper(\n",
    "                all_sequences['X'], all_sequences['y'], random_seed=balance_random_seed\n",
    "            )\n",
    "            print(f\"Balanced data shapes: X={all_sequences['X'].shape}, y={all_sequences['y'].shape}. New counts: {dict(zip(*np.unique(all_sequences['y'], return_counts=True)))}\")\n",
    "\n",
    "        return all_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f5c65cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 12\n",
    "STEP_SIZE = 1\n",
    "TARGET_COLUMN = 'M'\n",
    "NUMERICAL_COLS = [\n",
    "    \"FH\", \"FF\", \"FX\", \"T\", \"T10N\", \"TD\", \"SQ\", \"Q\", \"DR\", \"RH\", \"P\", \"U\", \n",
    "    # \"DD\"\n",
    "]\n",
    "CATEGORICAL_COLS = {\n",
    "    # \"WW\", \"IX\", \"VV\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bad45dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing all data... Original counts: {np.float64(0.0): np.int64(35194), np.float64(1.0): np.int64(1839)}\n",
      "Balanced data shapes: X=(3678, 12, 12), y=(3678,). New counts: {np.float64(0.0): np.int64(1839), np.float64(1.0): np.int64(1839)}\n"
     ]
    }
   ],
   "source": [
    "train_data = create_sequences_for_classification(\n",
    "    df=df, \n",
    "    target_column_name=TARGET_COLUMN, \n",
    "    numerical_features_cols=NUMERICAL_COLS, \n",
    "    categorical_features_cols=CATEGORICAL_COLS, \n",
    "    prev_time_steps=SEQUENCE_LENGTH,\n",
    "    balance_data=True,\n",
    "    balance_random_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "96ea186f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df_features shape: (37045, 23)\n",
      "Train data shape (after target NaN drop, before sequence creation): (22581, 23)\n",
      "Test data shape (after target NaN drop, before sequence creation): (12920, 23)\n",
      "Balancing training data... Original counts: {np.float64(0.0): np.int64(21531), np.float64(1.0): np.int64(1038)}\n",
      "Balanced training data shapes: X=(2076, 12, 12), y=(2076,). New counts: {np.float64(0.0): np.int64(1038), np.float64(1.0): np.int64(1038)}\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = create_sequences_for_classification(\n",
    "    df=df, \n",
    "    target_column_name=TARGET_COLUMN, \n",
    "    numerical_features_cols=NUMERICAL_COLS, \n",
    "    categorical_features_cols=CATEGORICAL_COLS, \n",
    "    prev_time_steps=SEQUENCE_LENGTH,\n",
    "    balance_data=True,\n",
    "    balance_random_seed=42,\n",
    "    split_date=pd.Timestamp(year=2022, month=1, day=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "301a6f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training X shape: (2076, 12, 12)\n",
      "Final Training y shape: (2076,)\n",
      "Training y distribution: (array([0., 1.]), array([1038, 1038]))\n",
      "Final Test X shape: (1330, 12, 12)\n",
      "Final Test y shape: (1330,)\n",
      "Test y distribution: (array([0., 1.]), array([665, 665]))\n"
     ]
    }
   ],
   "source": [
    "if train_data and train_data[\"X\"].size > 0:\n",
    "    print(f\"Final Training X shape: {train_data['X'].shape}\")\n",
    "    print(f\"Final Training y shape: {train_data['y'].shape}\")\n",
    "    y_int = train_data['y']\n",
    "    print(f\"Training y distribution: {np.unique(y_int, equal_nan=True, return_counts=True)}\")\n",
    "else:\n",
    "    print(\"Training data is empty or could not be generated.\")\n",
    "\n",
    "if test_data and test_data[\"X\"].size > 0:\n",
    "    print(f\"Final Test X shape: {test_data['X'].shape}\")\n",
    "    print(f\"Final Test y shape: {test_data['y'].shape}\")\n",
    "    y_int = test_data['y']\n",
    "    print(f\"Test y distribution: {np.unique(y_int, equal_nan=True, return_counts=True)}\")\n",
    "else:\n",
    "    print(\"Test data is empty or could not be generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f2a96",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f9ac0",
   "metadata": {},
   "source": [
    "## Raindrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "efaf9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 21:39:25 [INFO]: No given device, using default device: cuda\n",
      "2025-05-21 21:39:25 [INFO]: Model files will be saved to ./runs/classify/WEATHER-KNMI/raindrop/20250521_T213925\n",
      "2025-05-21 21:39:25 [INFO]: Tensorboard file will be saved to ./runs/classify/WEATHER-KNMI/raindrop/20250521_T213925/tensorboard\n",
      "2025-05-21 21:39:25 [INFO]: Using customized CrossEntropy as the training loss function.\n",
      "2025-05-21 21:39:25 [INFO]: Using customized CrossEntropy as the validation metric function.\n",
      "2025-05-21 21:39:25 [INFO]: Raindrop initialized with the given hyperparameters, the number of trainable parameters: 173,636\n"
     ]
    }
   ],
   "source": [
    "raindrop = Raindrop(\n",
    "    n_steps=train_data['X'].shape[1],\n",
    "    n_features=train_data['X'].shape[2],\n",
    "    n_classes=2,\n",
    "    n_layers=2,\n",
    "    d_model=train_data['X'].shape[2] * 4,\n",
    "    d_ffn=256,\n",
    "    n_heads=2,\n",
    "    dropout=0.3,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    patience=3,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=None,\n",
    "    saving_path='./runs/classify/WEATHER-KNMI/raindrop',\n",
    "    model_saving_strategy='best'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68d36063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 21:39:38 [INFO]: Epoch 001 - training loss (CrossEntropy): 0.6976, validation CrossEntropy: 0.6959\n",
      "2025-05-21 21:39:48 [INFO]: Epoch 002 - training loss (CrossEntropy): 0.6946, validation CrossEntropy: 0.6930\n",
      "2025-05-21 21:39:57 [INFO]: Epoch 003 - training loss (CrossEntropy): 0.6931, validation CrossEntropy: 0.6670\n",
      "2025-05-21 21:40:08 [INFO]: Epoch 004 - training loss (CrossEntropy): 0.6905, validation CrossEntropy: 0.6892\n",
      "2025-05-21 21:40:18 [INFO]: Epoch 005 - training loss (CrossEntropy): 0.6448, validation CrossEntropy: 0.5064\n",
      "2025-05-21 21:40:29 [INFO]: Epoch 006 - training loss (CrossEntropy): 0.4857, validation CrossEntropy: 0.3308\n",
      "2025-05-21 21:40:40 [INFO]: Epoch 007 - training loss (CrossEntropy): 0.4191, validation CrossEntropy: 0.3190\n",
      "2025-05-21 21:40:51 [INFO]: Epoch 008 - training loss (CrossEntropy): 0.4201, validation CrossEntropy: 0.3058\n",
      "2025-05-21 21:41:01 [INFO]: Epoch 009 - training loss (CrossEntropy): 0.3943, validation CrossEntropy: 0.2951\n",
      "2025-05-21 21:41:11 [INFO]: Epoch 010 - training loss (CrossEntropy): 0.3820, validation CrossEntropy: 0.3145\n",
      "2025-05-21 21:41:20 [INFO]: Epoch 011 - training loss (CrossEntropy): 0.3858, validation CrossEntropy: 0.2921\n",
      "2025-05-21 21:41:30 [INFO]: Epoch 012 - training loss (CrossEntropy): 0.3775, validation CrossEntropy: 0.2846\n",
      "2025-05-21 21:41:40 [INFO]: Epoch 013 - training loss (CrossEntropy): 0.3655, validation CrossEntropy: 0.2749\n",
      "2025-05-21 21:41:50 [INFO]: Epoch 014 - training loss (CrossEntropy): 0.3673, validation CrossEntropy: 0.2948\n",
      "2025-05-21 21:42:00 [INFO]: Epoch 015 - training loss (CrossEntropy): 0.3518, validation CrossEntropy: 0.2827\n",
      "2025-05-21 21:42:09 [INFO]: Epoch 016 - training loss (CrossEntropy): 0.3585, validation CrossEntropy: 0.2777\n",
      "2025-05-21 21:42:09 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-21 21:42:09 [INFO]: Finished training. The best model is from epoch#13.\n",
      "2025-05-21 21:42:09 [INFO]: Saved the model to ./runs/classify/WEATHER-KNMI/raindrop/20250521_T213925/Raindrop.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.8849624060150375, \n",
      "PR_AUC: 0.9074526868351983,\n",
      "F1: 0.8927820602662929,\n",
      "Precision: 0.8359580052493438,\n",
      "Recall: 0.9578947368421052,\n",
      "Accuracy: 0.8849624060150376\n"
     ]
    }
   ],
   "source": [
    "raindrop.fit(train_set=train_data, val_set=test_data)\n",
    "results = raindrop.predict(test_data)\n",
    "prediction = results['classification']\n",
    "metrics = calc_binary_classification_metrics(prediction, test_data['y'])\n",
    "print(\"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    "    f'Accuracy: {metrics[\"accuracy\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf20327",
   "metadata": {},
   "source": [
    "## BRITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0847e160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 21:42:12 [INFO]: No given device, using default device: cuda\n",
      "2025-05-21 21:42:12 [INFO]: Model files will be saved to ./runs/classify/WEATHER-KNMI/brits/20250521_T214212\n",
      "2025-05-21 21:42:12 [INFO]: Tensorboard file will be saved to ./runs/classify/WEATHER-KNMI/brits/20250521_T214212/tensorboard\n",
      "2025-05-21 21:42:12 [INFO]: Using customized CrossEntropy as the training loss function.\n",
      "2025-05-21 21:42:12 [INFO]: Using customized CrossEntropy as the validation metric function.\n",
      "2025-05-21 21:42:12 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 592,612\n"
     ]
    }
   ],
   "source": [
    "brits = BRITS(\n",
    "    n_steps=train_data['X'].shape[1],\n",
    "    n_features=train_data['X'].shape[2],\n",
    "    n_classes=2,\n",
    "    rnn_hidden_size=256,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    patience=3,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=None,\n",
    "    saving_path='./runs/classify/WEATHER-KNMI/brits',\n",
    "    model_saving_strategy='best'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "735b1014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 21:42:32 [INFO]: Epoch 001 - training loss (CrossEntropy): 2901.0337, validation CrossEntropy: 0.6801\n",
      "2025-05-21 21:42:51 [INFO]: Epoch 002 - training loss (CrossEntropy): 2373.2363, validation CrossEntropy: 0.6686\n",
      "2025-05-21 21:43:06 [INFO]: Epoch 003 - training loss (CrossEntropy): 2074.8020, validation CrossEntropy: 0.6677\n",
      "2025-05-21 21:43:21 [INFO]: Epoch 004 - training loss (CrossEntropy): 1931.4504, validation CrossEntropy: 0.6531\n",
      "2025-05-21 21:43:37 [INFO]: Epoch 005 - training loss (CrossEntropy): 1874.9596, validation CrossEntropy: 0.6409\n",
      "2025-05-21 21:43:58 [INFO]: Epoch 006 - training loss (CrossEntropy): 1862.3724, validation CrossEntropy: 0.6391\n",
      "2025-05-21 21:44:17 [INFO]: Epoch 007 - training loss (CrossEntropy): 1855.3852, validation CrossEntropy: 0.6266\n",
      "2025-05-21 21:44:36 [INFO]: Epoch 008 - training loss (CrossEntropy): 1848.6624, validation CrossEntropy: 0.6175\n",
      "2025-05-21 21:44:54 [INFO]: Epoch 009 - training loss (CrossEntropy): 1842.3058, validation CrossEntropy: 0.6124\n",
      "2025-05-21 21:45:14 [INFO]: Epoch 010 - training loss (CrossEntropy): 1836.1539, validation CrossEntropy: 0.6210\n",
      "2025-05-21 21:45:34 [INFO]: Epoch 011 - training loss (CrossEntropy): 1830.1355, validation CrossEntropy: 0.6004\n",
      "2025-05-21 21:45:54 [INFO]: Epoch 012 - training loss (CrossEntropy): 1824.3533, validation CrossEntropy: 0.6088\n",
      "2025-05-21 21:46:21 [INFO]: Epoch 013 - training loss (CrossEntropy): 1818.7659, validation CrossEntropy: 0.5916\n",
      "2025-05-21 21:47:09 [INFO]: Epoch 014 - training loss (CrossEntropy): 1813.3129, validation CrossEntropy: 0.5872\n",
      "2025-05-21 21:47:27 [INFO]: Epoch 015 - training loss (CrossEntropy): 1807.9233, validation CrossEntropy: 0.5779\n",
      "2025-05-21 21:47:48 [INFO]: Epoch 016 - training loss (CrossEntropy): 1802.5182, validation CrossEntropy: 0.5759\n",
      "2025-05-21 21:48:11 [INFO]: Epoch 017 - training loss (CrossEntropy): 1797.1697, validation CrossEntropy: 0.5710\n",
      "2025-05-21 21:48:34 [INFO]: Epoch 018 - training loss (CrossEntropy): 1791.7170, validation CrossEntropy: 0.5715\n",
      "2025-05-21 21:48:58 [INFO]: Epoch 019 - training loss (CrossEntropy): 1786.3215, validation CrossEntropy: 0.5800\n",
      "2025-05-21 21:49:22 [INFO]: Epoch 020 - training loss (CrossEntropy): 1780.8330, validation CrossEntropy: 0.5645\n",
      "2025-05-21 21:49:22 [INFO]: Finished training. The best model is from epoch#20.\n",
      "2025-05-21 21:49:22 [INFO]: Saved the model to ./runs/classify/WEATHER-KNMI/brits/20250521_T214212/BRITS.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.9007518796992481, \n",
      "PR_AUC: 0.9232825513565572,\n",
      "F1: 0.9023668639053254,\n",
      "Precision: 0.8879184861717613,\n",
      "Recall: 0.9172932330827067,\n",
      "Accuracy: 0.9007518796992481\n"
     ]
    }
   ],
   "source": [
    "brits.fit(train_set=train_data, val_set=test_data)\n",
    "results = brits.predict(test_data)\n",
    "prediction = results['classification']\n",
    "metrics = calc_binary_classification_metrics(prediction, test_data['y'])\n",
    "print(\"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    "    f'Accuracy: {metrics[\"accuracy\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b8222",
   "metadata": {},
   "source": [
    "## Not deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "133ce0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import ClassifierMixin\n",
    "from typing import Any, TypeVar\n",
    "from collections import namedtuple\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ba30246",
   "metadata": {},
   "outputs": [],
   "source": [
    "_T = TypeVar('_T', bound=ClassifierMixin)\n",
    "\n",
    "def evaluate_model(model: ClassifierMixin, X: Any, y: Any):\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred, average='macro')\n",
    "    precision = precision_score(y, y_pred, average='macro')\n",
    "    recall = recall_score(y, y_pred, average='macro')\n",
    "    confusion = confusion_matrix(y, y_pred)\n",
    "    return namedtuple('Evaluation', ['accuracy', 'f1', 'precision', 'recall', 'confusion'])(accuracy, f1, precision, recall, confusion)\n",
    "\n",
    "def train_model(\n",
    "        model_cls: _T, \n",
    "        model_kwargs: dict[str, Any],\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        X_test: np.ndarray,\n",
    "        y_test: np.ndarray\n",
    "    ) -> _T:\n",
    "    model = model_cls(**model_kwargs)\n",
    "    if X_train.ndim == 3:\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "    if X_test.ndim == 3:\n",
    "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1] * X_test.shape[2]))\n",
    "\n",
    "    X_train = np.nan_to_num(X_train, nan=-1.0)\n",
    "    X_test = np.nan_to_num(X_test, nan=-1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_metrics = evaluate_model(model, X_train, y_train)\n",
    "    val_metrics = evaluate_model(model, X_test, y_test)\n",
    "    print(f\"Model - {model_cls.__name__}\")\n",
    "    print(\"\\tTrain metrics:\")\n",
    "    print(f\"\\t\\tAccuracy: {train_metrics.accuracy:.4f}\")\n",
    "    print(f\"\\t\\tF1: {train_metrics.f1:.4f}\")\n",
    "    print(f\"\\t\\tPrecision: {train_metrics.precision:.4f}\")\n",
    "    print(f\"\\t\\tRecall: {train_metrics.recall:.4f}\")\n",
    "    print(\"\\tValidation metrics:\")\n",
    "    print(f\"\\t\\tAccuracy: {val_metrics.accuracy:.4f}\")\n",
    "    print(f\"\\t\\tF1: {val_metrics.f1:.4f}\")\n",
    "    print(f\"\\t\\tPrecision: {val_metrics.precision:.4f}\")\n",
    "    print(f\"\\t\\tRecall: {val_metrics.recall:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "891be840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - SVC\n",
      "\tTrain metrics:\n",
      "\t\tAccuracy: 0.6802\n",
      "\t\tF1: 0.6641\n",
      "\t\tPrecision: 0.7228\n",
      "\t\tRecall: 0.6802\n",
      "\tValidation metrics:\n",
      "\t\tAccuracy: 0.6857\n",
      "\t\tF1: 0.6606\n",
      "\t\tPrecision: 0.7639\n",
      "\t\tRecall: 0.6857\n"
     ]
    }
   ],
   "source": [
    "svc = train_model(\n",
    "    SVC,\n",
    "    {},\n",
    "    X_train=train_data[\"X\"],\n",
    "    y_train=train_data[\"y\"],\n",
    "    X_test=test_data[\"X\"],\n",
    "    y_test=test_data[\"y\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "973810c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - RandomForestClassifier\n",
      "\tTrain metrics:\n",
      "\t\tAccuracy: 1.0000\n",
      "\t\tF1: 1.0000\n",
      "\t\tPrecision: 1.0000\n",
      "\t\tRecall: 1.0000\n",
      "\tValidation metrics:\n",
      "\t\tAccuracy: 0.9459\n",
      "\t\tF1: 0.9459\n",
      "\t\tPrecision: 0.9461\n",
      "\t\tRecall: 0.9459\n"
     ]
    }
   ],
   "source": [
    "rfc = train_model(\n",
    "    RandomForestClassifier,\n",
    "    {},\n",
    "    X_train=train_data[\"X\"],\n",
    "    y_train=train_data[\"y\"],\n",
    "    X_test=test_data[\"X\"],\n",
    "    y_test=test_data[\"y\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f42a7d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - XGBClassifier\n",
      "\tTrain metrics:\n",
      "\t\tAccuracy: 1.0000\n",
      "\t\tF1: 1.0000\n",
      "\t\tPrecision: 1.0000\n",
      "\t\tRecall: 1.0000\n",
      "\tValidation metrics:\n",
      "\t\tAccuracy: 0.9308\n",
      "\t\tF1: 0.9307\n",
      "\t\tPrecision: 0.9335\n",
      "\t\tRecall: 0.9308\n"
     ]
    }
   ],
   "source": [
    "xgb = train_model(\n",
    "    XGBClassifier,\n",
    "    {},\n",
    "    X_train=train_data[\"X\"],\n",
    "    y_train=train_data[\"y\"],\n",
    "    X_test=test_data[\"X\"],\n",
    "    y_test=test_data[\"y\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
