{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3efc4a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from pypots.utils.random import set_random_seed\n",
    "from pypots.optim import Adam\n",
    "from pypots.classification import Raindrop, BRITS, GRUD\n",
    "from pypots.nn.functional import calc_binary_classification_metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783352d",
   "metadata": {},
   "source": [
    "# Prepare df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e475cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations = '249','323', '377'\n",
    "stations = '323',\n",
    "test_station = '215'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d560178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_vv_to_meters(vv_code):\n",
    "    if pd.isna(vv_code):\n",
    "        return np.nan\n",
    "    \n",
    "    vv_code = int(vv_code)\n",
    "\n",
    "    if 0 <= vv_code <= 49:\n",
    "        return vv_code * 100 + 50\n",
    "    elif vv_code == 50:\n",
    "        return 5500\n",
    "    elif 51 <= vv_code <= 55:\n",
    "        return np.nan\n",
    "    elif 56 <= vv_code <= 79:\n",
    "        return int((vv_code - 56 + 6.5) * 1000)\n",
    "    elif vv_code == 80:\n",
    "        return 32500\n",
    "    elif 81 <= vv_code <= 88:\n",
    "        return int(32500 + (vv_code - 81) * 5000)\n",
    "    elif vv_code == 89:\n",
    "        return 70000\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def _get_valid_vv_codes() -> list[int]:\n",
    "    valid_codes = list(range(0, 51))\n",
    "    valid_codes += list(range(56, 90))\n",
    "    return valid_codes\n",
    "\n",
    "def get_vv_one_hot_encoder() -> OneHotEncoder:\n",
    "    valid_codes = _get_valid_vv_codes()\n",
    "    categories = [np.array(valid_codes, dtype=np.int32)]\n",
    "    encoder = OneHotEncoder(categories=categories, handle_unknown='ignore', dtype=np.float32, sparse_output=False)\n",
    "    encoder.fit(categories[0].reshape(-1, 1))\n",
    "    return encoder\n",
    "\n",
    "def prepare_df(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        header_line_index = -1\n",
    "        column_names = []\n",
    "        data_lines_start_index = -1\n",
    "\n",
    "        # Find the header and its index more efficiently\n",
    "        with open(path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if line.strip().startswith('# STN,YYYYMMDD,'):\n",
    "                    header_line_index = i\n",
    "                    column_names = [col.strip() for col in line.strip().lstrip('#').split(',')]\n",
    "                    data_lines_start_index = header_line_index + 1\n",
    "                    break\n",
    "        \n",
    "        if header_line_index == -1:\n",
    "            raise ValueError(\"Header line not found.\")\n",
    "\n",
    "        # Use pandas.read_csv directly with skiprows and comment character\n",
    "        # This avoids reading the whole file into a list first for data lines\n",
    "        # and then joining them back.\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            names=column_names,\n",
    "            skiprows=data_lines_start_index,\n",
    "            comment='#',  # Lines starting with '#' will be ignored as comments\n",
    "            skipinitialspace=True,\n",
    "            na_values=['       ', '     '] # Add other common missing value representations if needed\n",
    "        )\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"No data found after the header or all data was commented out.\")\n",
    "\n",
    "        # Convert 'HH' to string and zfill, then create 'Timestamp'\n",
    "        # It's crucial to handle potential NaN values in 'YYYYMMDD' or 'HH'\n",
    "        # if they are not guaranteed to be present or valid in all rows.\n",
    "        df['HH'] = df['HH'].astype(int) - 1\n",
    "        df['HH'] = df['HH'].astype(str).str.zfill(2)\n",
    "        df['Timestamp'] = pd.to_datetime(df['YYYYMMDD'].astype(str) + df['HH'].astype(str), format=\"%Y%m%d%H\", errors='coerce')\n",
    "        \n",
    "        df.set_index('Timestamp', inplace=True)\n",
    "        \n",
    "        # Columns to drop\n",
    "        cols_to_drop = ['YYYYMMDD', 'HH']\n",
    "        df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "        # Convert remaining columns to numeric, efficiently\n",
    "        # Identify numeric columns once and convert\n",
    "        # Exclude already processed or known non-numeric columns if necessary\n",
    "        for col in df.columns:\n",
    "            # This check is slightly redundant if YYYYMMDD and HH are already dropped,\n",
    "            # but good for safety if they weren't or if other non-numeric columns exist.\n",
    "            if df[col].dtype == 'object': # Only attempt conversion if the column is of object type\n",
    "                try:\n",
    "                    df[col] = pd.to_numeric(df[col], downcast='signed')\n",
    "                except ValueError:\n",
    "                    # Handle or log cases where a column expected to be numeric isn't\n",
    "                    # For now, we'll coerce, which turns unparseable into NaT/NaN\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce', downcast='signed')\n",
    "        df['VV_m'] = df['VV'].apply(_convert_vv_to_meters)\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{path}' was not found.\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bdccf26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>STN</th>\n",
       "      <th>DD</th>\n",
       "      <th>FH</th>\n",
       "      <th>FF</th>\n",
       "      <th>FX</th>\n",
       "      <th>T</th>\n",
       "      <th>T10N</th>\n",
       "      <th>TD</th>\n",
       "      <th>SQ</th>\n",
       "      <th>...</th>\n",
       "      <th>N</th>\n",
       "      <th>U</th>\n",
       "      <th>WW</th>\n",
       "      <th>IX</th>\n",
       "      <th>M</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>O</th>\n",
       "      <th>Y</th>\n",
       "      <th>VV_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>323</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>323</td>\n",
       "      <td>270.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>323</td>\n",
       "      <td>260.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>323</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>323</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4350.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp  STN     DD    FH    FF    FX     T  T10N    TD   SQ  \\\n",
       "0 2011-01-01 00:00:00  323  250.0  30.0  40.0  50.0  40.0   NaN  39.0  0.0   \n",
       "1 2011-01-01 01:00:00  323  270.0  40.0  40.0  50.0  41.0   NaN  40.0  0.0   \n",
       "2 2011-01-01 02:00:00  323  260.0  30.0  30.0  40.0  40.0   NaN  39.0  0.0   \n",
       "3 2011-01-01 03:00:00  323  250.0  30.0  30.0  50.0  40.0   NaN  39.0  0.0   \n",
       "4 2011-01-01 04:00:00  323  250.0  30.0  40.0  50.0  40.0   NaN  39.0  0.0   \n",
       "\n",
       "   ...    N     U    WW  IX    M    R    S    O    Y    VV_m  \n",
       "0  ...  7.0  99.0  32.0   7  1.0  0.0  0.0  0.0  0.0   150.0  \n",
       "1  ...  8.0  99.0  20.0   7  1.0  0.0  0.0  0.0  0.0  2550.0  \n",
       "2  ...  8.0  99.0  10.0   7  0.0  0.0  0.0  0.0  0.0  6500.0  \n",
       "3  ...  8.0  99.0  10.0   7  0.0  0.0  0.0  0.0  0.0  5500.0  \n",
       "4  ...  8.0  99.0  22.0   7  0.0  1.0  0.0  0.0  0.0  4350.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for station in stations:\n",
    "    df = prepare_df(f\"./datasets/knmi_station_data/{station}.txt\")\n",
    "    df = df.set_index('STN', append=True)\n",
    "    # Check whether VV column has any noy nulls\n",
    "    nulls = df['VV'].isna().sum()\n",
    "    dfs.append(df)\n",
    "\n",
    "train_df = pd.concat(dfs)\n",
    "train_df = df.reset_index()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845b4bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>STN</th>\n",
       "      <th>DD</th>\n",
       "      <th>FH</th>\n",
       "      <th>FF</th>\n",
       "      <th>FX</th>\n",
       "      <th>T</th>\n",
       "      <th>T10N</th>\n",
       "      <th>TD</th>\n",
       "      <th>SQ</th>\n",
       "      <th>...</th>\n",
       "      <th>N</th>\n",
       "      <th>U</th>\n",
       "      <th>WW</th>\n",
       "      <th>IX</th>\n",
       "      <th>M</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>O</th>\n",
       "      <th>Y</th>\n",
       "      <th>VV_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>215</td>\n",
       "      <td>210.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>215</td>\n",
       "      <td>220.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>215</td>\n",
       "      <td>200.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>215</td>\n",
       "      <td>210.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>215</td>\n",
       "      <td>190.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp  STN     DD    FH    FF    FX   T  T10N  TD  SQ  ...  \\\n",
       "0 2015-01-01 00:00:00  215  210.0  50.0  50.0  70.0  27   NaN   8   0  ...   \n",
       "1 2015-01-01 01:00:00  215  220.0  50.0  50.0  70.0  26   NaN   4   0  ...   \n",
       "2 2015-01-01 02:00:00  215  200.0  50.0  40.0  80.0  23   NaN   2   0  ...   \n",
       "3 2015-01-01 03:00:00  215  210.0  40.0  40.0  70.0  21   NaN   1   0  ...   \n",
       "4 2015-01-01 04:00:00  215  190.0  50.0  50.0  80.0  19   NaN   2   0  ...   \n",
       "\n",
       "     N   U    WW  IX    M    R    S    O    Y     VV_m  \n",
       "0  0.0  87  10.0   7  0.0  0.0  0.0  0.0  0.0   4250.0  \n",
       "1  0.0  85  10.0   7  0.0  0.0  0.0  0.0  0.0   7500.0  \n",
       "2  0.0  86   NaN   5  0.0  0.0  0.0  0.0  0.0  10500.0  \n",
       "3  0.0  87   NaN   5  0.0  0.0  0.0  0.0  0.0  10500.0  \n",
       "4  1.0  88   NaN   5  0.0  0.0  0.0  0.0  0.0  10500.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for station in [test_station]:\n",
    "    df = prepare_df(f\"./datasets/knmi_station_data/{station}.txt\")\n",
    "    df = df.set_index('STN', append=True)\n",
    "    # Check whether VV column has any noy nulls\n",
    "    nulls = df['VV'].isna().sum()\n",
    "    dfs.append(df)\n",
    "\n",
    "test_df = pd.concat(dfs)\n",
    "test_df = df.reset_index()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc73e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for core processing and sequence creation\n",
    "def _process_and_create_sequences_internal(\n",
    "        df_segment: pd.DataFrame,\n",
    "        target_column_name: str,\n",
    "        numerical_features_cols: list[str],\n",
    "        categorical_features_cols: dict[str, OneHotEncoder],\n",
    "        prev_time_steps: int,\n",
    "        timestamp_column: str = 'Timestamp',\n",
    "        expected_time_interval: pd.Timedelta = pd.Timedelta(hours=1)\n",
    ") -> dict[str, np.ndarray]:\n",
    "\n",
    "    # This df_segment is assumed to have target_column_name NaNs already handled if desired.\n",
    "    \n",
    "    processed_feature_dfs_list = []\n",
    "\n",
    "    # 1. Process Numerical Features\n",
    "    if numerical_features_cols:\n",
    "        valid_numerical_cols = [col for col in numerical_features_cols if col in df_segment.columns]\n",
    "        if valid_numerical_cols: # Only proceed if there are valid numerical columns to select\n",
    "            numerical_df = df_segment[valid_numerical_cols].copy()\n",
    "            processed_feature_dfs_list.append(numerical_df)\n",
    "        elif not valid_numerical_cols and numerical_features_cols: # Specified but none found\n",
    "            print(f\"Warning (internal): None of the specified numerical features found in the current data segment. Numerical features count: {len(numerical_features_cols)}\")\n",
    "\n",
    "\n",
    "    # 2. Process Categorical Features\n",
    "    for col_name, encoder in categorical_features_cols.items():\n",
    "        if col_name not in df_segment.columns:\n",
    "            # This happens if df_segment is empty or the column was dropped.\n",
    "            # The impact on num_actual_features will be handled later.\n",
    "            continue\n",
    "\n",
    "        column_to_encode = df_segment[[col_name]]\n",
    "        encoded_data_sparse = np.array([]) # Default empty\n",
    "        \n",
    "        if column_to_encode.empty: # Encoder might not handle empty DataFrame input well for transform if it expects rows\n",
    "             try: # Get feature names to create an empty DataFrame with correct OHE columns\n",
    "                ohe_feature_names = encoder.get_feature_names_out([col_name])\n",
    "             except AttributeError:\n",
    "                ohe_feature_names = encoder.get_feature_names([col_name])\n",
    "             except Exception:\n",
    "                ohe_feature_names = [f\"{col_name}_cat{i}\" for i in range(len(encoder.categories_[0]))] if hasattr(encoder, 'categories_') else [f\"{col_name}_unknown_cat\"]\n",
    "             \n",
    "             # Create an empty DataFrame (0 rows) with the OHE column names\n",
    "             ohe_df = pd.DataFrame(columns=ohe_feature_names, index=df_segment.index, dtype=float)\n",
    "             processed_feature_dfs_list.append(ohe_df)\n",
    "             continue # Go to next categorical column\n",
    "\n",
    "        # If not empty, proceed with transform\n",
    "        encoded_data_sparse = encoder.transform(column_to_encode)\n",
    "        \n",
    "        try:\n",
    "            ohe_feature_names = encoder.get_feature_names_out([col_name])\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                ohe_feature_names = encoder.get_feature_names([col_name])\n",
    "            except TypeError:\n",
    "                ohe_feature_names = [f\"{col_name}_{category}\" for category in encoder.categories_[0]]\n",
    "            except Exception as e_fn: # Broad exception for get_feature_names issues\n",
    "                print(f\"Warning (internal): Could not reliably get OHE feature names for '{col_name}'. Using generic names. Error: {e_fn}\")\n",
    "                num_output_features = encoded_data_sparse.shape[1]\n",
    "                ohe_feature_names = [f\"{col_name}_ohe_{i}\" for i in range(num_output_features)]\n",
    "\n",
    "        if hasattr(encoded_data_sparse, \"toarray\"):\n",
    "            encoded_data_dense = encoded_data_sparse.toarray()\n",
    "        else:\n",
    "            encoded_data_dense = encoded_data_sparse\n",
    "        \n",
    "        if encoded_data_dense.shape[0] > 0 : # Ensure data was actually produced\n",
    "            ohe_df = pd.DataFrame(encoded_data_dense, columns=ohe_feature_names, index=df_segment.index)\n",
    "            processed_feature_dfs_list.append(ohe_df)\n",
    "        elif df_segment.shape[0] > 0 : # Input segment had rows, but OHE produced no rows (should not happen with sklearn)\n",
    "             print(f\"Warning (internal): OHE for '{col_name}' produced 0 rows from a non-empty segment. Check encoder.\")\n",
    "\n",
    "\n",
    "    # 3. Combine all processed feature DataFrames\n",
    "    num_actual_features = 0\n",
    "    if not processed_feature_dfs_list:\n",
    "        final_features_df = pd.DataFrame(index=df_segment.index) # 0 columns\n",
    "        if numerical_features_cols or categorical_features_cols:\n",
    "             print(\"Warning (internal): No features were processed into final_features_df despite specification. X will have 0 features for this segment.\")\n",
    "    else:\n",
    "        final_features_df = pd.concat(processed_feature_dfs_list, axis=1)\n",
    "    num_actual_features = final_features_df.shape[1]\n",
    "\n",
    "    # 4. Fill missing values\n",
    "    final_features_df[timestamp_column] = df_segment[timestamp_column]\n",
    "    final_features_df[\"target\"] = df_segment[target_column_name]\n",
    "    final_features_df = final_features_df.sort_values(timestamp_column).reset_index(drop=True)\n",
    "\n",
    "    features_only = final_features_df.drop(columns=[\"target\"])\n",
    "    feature_cols = features_only.columns.difference([timestamp_column])\n",
    "    feature_lookup = features_only.set_index(timestamp_column)\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(prev_time_steps, len(final_features_df)):\n",
    "        current_time = final_features_df.loc[i, timestamp_column]\n",
    "        expected_times = [current_time - j * expected_time_interval for j in range(prev_time_steps, 0, -1)]\n",
    "\n",
    "        sequence_rows = []\n",
    "        for ts in expected_times:\n",
    "            if ts in feature_lookup.index:\n",
    "                row = feature_lookup.loc[ts][feature_cols].values\n",
    "            else:\n",
    "                row = np.full(len(feature_cols), np.nan)\n",
    "            sequence_rows.append(row)\n",
    "        \n",
    "        feature_sequence = np.vstack(sequence_rows)\n",
    "        x_list.append(feature_sequence)\n",
    "        target_value = final_features_df.loc[i, \"target\"]\n",
    "        y_list.append(target_value)\n",
    "\n",
    "    num_actual_features = len(feature_cols)\n",
    "    if not x_list:\n",
    "        return {\n",
    "            'X': np.array([]).reshape(0, prev_time_steps, num_actual_features),\n",
    "            'y': np.array([])\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'X': np.array(x_list),\n",
    "        'y': np.array(y_list)\n",
    "    }\n",
    "\n",
    "    # --- Sequence Creation ---\n",
    "    # X_list = []\n",
    "    # y_list = []\n",
    "    # num_rows_in_features = len(final_features_df)\n",
    "\n",
    "    # if prev_time_steps >= num_rows_in_features :\n",
    "    #     return {'X': np.array([]).reshape(0, prev_time_steps, num_actual_features), 'y': np.array([])}\n",
    "\n",
    "    # y_series_segment = df_segment[target_column_name]\n",
    "\n",
    "\n",
    "\n",
    "    # for i in range(prev_time_steps, num_rows_in_features):\n",
    "    #     feature_sequence = final_features_df.iloc[i - prev_time_steps : i].values\n",
    "    #     X_list.append(feature_sequence)\n",
    "    #     target_value = y_series_segment.iloc[i]\n",
    "    #     y_list.append(target_value)\n",
    "\n",
    "    # if not X_list:\n",
    "    #     return {'X': np.array([]).reshape(0, prev_time_steps, num_actual_features), 'y': np.array([])}\n",
    "        \n",
    "    # X_np = np.array(X_list)\n",
    "    # y_np = np.array(y_list)\n",
    "    \n",
    "    # return {'X': X_np, 'y': y_np}\n",
    "\n",
    "# Helper for balancing data (undersampling)\n",
    "def _balance_data_helper(X_in: np.ndarray, y_in: np.ndarray, random_seed: int | None = None, max_other_ratio: float = 2) -> tuple[np.ndarray, np.ndarray]:\n",
    "    if X_in.shape[0] == 0: # No data to balance\n",
    "        return X_in, y_in\n",
    "\n",
    "    unique_classes, counts = np.unique(y_in, return_counts=True)\n",
    "    \n",
    "    if len(unique_classes) <= 1: # Already balanced or only one class\n",
    "        return X_in, y_in\n",
    "\n",
    "    min_count = np.min(counts)\n",
    "    \n",
    "    balanced_indices_list = []\n",
    "    rng = np.random.RandomState(random_seed) # For reproducible undersampling if seed is provided\n",
    "\n",
    "    for cls_val in unique_classes:\n",
    "        cls_indices = np.where(y_in == cls_val)[0]\n",
    "        cls_size = int(min_count * max_other_ratio)\n",
    "        cls_size = min(len(cls_indices), cls_size)\n",
    "        if len(cls_indices) > min_count:\n",
    "            chosen_indices = rng.choice(cls_indices, size=cls_size, replace=False)\n",
    "        else:\n",
    "            chosen_indices = cls_indices # Take all if it's already the min count or less\n",
    "        balanced_indices_list.extend(chosen_indices)\n",
    "    \n",
    "    # Shuffle the combined indices from different classes\n",
    "    shuffled_balanced_indices = rng.permutation(balanced_indices_list)\n",
    "    \n",
    "    return X_in[shuffled_balanced_indices], y_in[shuffled_balanced_indices]\n",
    "\n",
    "\n",
    "# Main Function\n",
    "def create_sequences_for_classification(\n",
    "        df: pd.DataFrame,\n",
    "        target_column_name: str,\n",
    "        numerical_features_cols: list[str],\n",
    "        categorical_features_cols: dict[str, OneHotEncoder], # {col_name: fitted_encoder}\n",
    "        prev_time_steps: int = 8,\n",
    "        split_date: str | pd.Timestamp | None = None,\n",
    "        balance_data: bool = False,\n",
    "        balance_random_seed: int | None = None, # Seed for balancing reproducibility\n",
    "        timestamp_column: str = 'Timestamp',\n",
    "        expected_time_interval: pd.Timedelta = pd.Timedelta(hours=1),\n",
    "        max_other_ratio: float = 2\n",
    ") -> dict[str, np.ndarray] | tuple[dict[str, np.ndarray], dict[str, np.ndarray]]:\n",
    "\n",
    "    if target_column_name not in df.columns: # Check on original df\n",
    "        raise ValueError(f\"Target column '{target_column_name}' not found in DataFrame.\")\n",
    "    \n",
    "    df_features = df.copy() # Use the name from your stub\n",
    "    # --- User requested modification: remove rows where target is NaN ---\n",
    "    df_features = df_features[~df_features[target_column_name].isna()]\n",
    "    # --- End of modification ---\n",
    "\n",
    "    if df_features.empty:\n",
    "        print(f\"DataFrame is empty after removing NaNs in target column '{target_column_name}'.\")\n",
    "        # The _process_and_create_sequences_internal can handle an empty df_features\n",
    "        # and return correctly shaped empty arrays.\n",
    "        empty_sequences = _process_and_create_sequences_internal(\n",
    "            df_features, target_column_name, numerical_features_cols,\n",
    "            categorical_features_cols, prev_time_steps\n",
    "        )\n",
    "        if split_date:\n",
    "            return empty_sequences, empty_sequences.copy() # Return two empty dicts\n",
    "        else:\n",
    "            return empty_sequences\n",
    "\n",
    "\n",
    "    # Basic Validations for other parameters\n",
    "    if not isinstance(prev_time_steps, int) or prev_time_steps <= 0:\n",
    "        raise ValueError(\"'prev_time_steps' must be a positive integer.\")\n",
    "    for col in numerical_features_cols:\n",
    "        if col not in df_features.columns: # Check on df_features as it's the one being processed\n",
    "            print(f\"Warning: Numerical feature column '{col}' not found in (cleaned) DataFrame. It will be ignored if missing.\")\n",
    "    for col in categorical_features_cols.keys():\n",
    "        if col not in df_features.columns:\n",
    "            print(f\"Warning: Categorical feature column '{col}' not found in (cleaned) DataFrame. It will be ignored if missing.\")\n",
    "    \n",
    "    if split_date:\n",
    "        split_date_ts = pd.to_datetime(split_date) # Converts str or pd.Timestamp\n",
    "\n",
    "        train_df = df_features[df_features[timestamp_column] <= split_date_ts].copy() # Use .copy() to avoid SettingWithCopyWarning later\n",
    "        test_df = df_features[df_features[timestamp_column] > split_date_ts].copy()\n",
    "\n",
    "        print(f\"Original df_features shape: {df_features.shape}\")\n",
    "        print(f\"Train data shape (after target NaN drop, before sequence creation): {train_df.shape}\")\n",
    "        print(f\"Test data shape (after target NaN drop, before sequence creation): {test_df.shape}\")\n",
    "\n",
    "        if train_df.empty:\n",
    "            print(\"Warning: Train DataFrame is empty after split.\")\n",
    "        if test_df.empty:\n",
    "            print(\"Warning: Test DataFrame is empty after split.\")\n",
    "\n",
    "        train_sequences = _process_and_create_sequences_internal(\n",
    "            train_df, target_column_name, numerical_features_cols,\n",
    "            categorical_features_cols, prev_time_steps, timestamp_column, expected_time_interval\n",
    "        )\n",
    "        test_sequences = _process_and_create_sequences_internal(\n",
    "            test_df, target_column_name, numerical_features_cols,\n",
    "            categorical_features_cols, prev_time_steps, timestamp_column, expected_time_interval\n",
    "        )\n",
    "\n",
    "        if balance_data and train_sequences['X'].shape[0] > 0:\n",
    "            print(f\"Balancing training data... Original counts: {dict(zip(*np.unique(train_sequences['y'], return_counts=True)))}\")\n",
    "            train_sequences['X'], train_sequences['y'] = _balance_data_helper(\n",
    "                train_sequences['X'], train_sequences['y'], random_seed=balance_random_seed\n",
    "            )\n",
    "            print(f\"Balanced training data shapes: X={train_sequences['X'].shape}, y={train_sequences['y'].shape}. New counts: {dict(zip(*np.unique(train_sequences['y'], return_counts=True)))}\")\n",
    "        if balance_data and test_sequences['X'].shape[0] > 0:\n",
    "            test_sequences['X'], test_sequences['y'] = _balance_data_helper(\n",
    "                test_sequences['X'], test_sequences['y'], random_seed=balance_random_seed\n",
    "            )\n",
    "        return train_sequences, test_sequences\n",
    "    else:\n",
    "        # No split, process the whole df_features\n",
    "        all_sequences = _process_and_create_sequences_internal(\n",
    "            df_features, target_column_name, numerical_features_cols,\n",
    "            categorical_features_cols, prev_time_steps\n",
    "        )\n",
    "        \n",
    "        if balance_data and all_sequences['X'].shape[0] > 0:\n",
    "            print(f\"Balancing all data... Original counts: {dict(zip(*np.unique(all_sequences['y'], return_counts=True)))}\")\n",
    "            all_sequences['X'], all_sequences['y'] = _balance_data_helper(\n",
    "                all_sequences['X'], all_sequences['y'], random_seed=balance_random_seed\n",
    "            )\n",
    "            print(f\"Balanced data shapes: X={all_sequences['X'].shape}, y={all_sequences['y'].shape}. New counts: {dict(zip(*np.unique(all_sequences['y'], return_counts=True)))}\")\n",
    "\n",
    "        return all_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c65cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 12\n",
    "STEP_SIZE = 1\n",
    "TARGET_COLUMN = 'VV'\n",
    "NUMERICAL_COLS = [\n",
    "    \"FH\", \"FF\", \"FX\", \"T\", \"T10N\", \"TD\", \"SQ\", \"Q\", \"DR\", \"RH\", \"P\", \"U\", \n",
    "    # \"DD\"\n",
    "]\n",
    "CATEGORICAL_COLS = {\n",
    "    # \"WW\", \"IX\", \"VV\"\n",
    "}\n",
    "\n",
    "vv_encoder = get_vv_one_hot_encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96ea186f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df_features shape: (10261, 25)\n",
      "Train data shape (after target NaN drop, before sequence creation): (6748, 25)\n",
      "Test data shape (after target NaN drop, before sequence creation): (3513, 25)\n",
      "Balancing training data... Original counts: {np.float64(0.0): np.int64(5), np.float64(1.0): np.int64(14), np.float64(2.0): np.int64(25), np.float64(3.0): np.int64(18), np.float64(4.0): np.int64(10), np.float64(5.0): np.int64(9), np.float64(6.0): np.int64(4), np.float64(7.0): np.int64(9), np.float64(8.0): np.int64(9), np.float64(9.0): np.int64(10), np.float64(10.0): np.int64(10), np.float64(11.0): np.int64(12), np.float64(12.0): np.int64(15), np.float64(13.0): np.int64(15), np.float64(14.0): np.int64(12), np.float64(15.0): np.int64(15), np.float64(16.0): np.int64(12), np.float64(17.0): np.int64(19), np.float64(18.0): np.int64(26), np.float64(19.0): np.int64(24), np.float64(20.0): np.int64(22), np.float64(21.0): np.int64(32), np.float64(22.0): np.int64(30), np.float64(23.0): np.int64(21), np.float64(24.0): np.int64(28), np.float64(25.0): np.int64(25), np.float64(26.0): np.int64(33), np.float64(27.0): np.int64(25), np.float64(28.0): np.int64(29), np.float64(29.0): np.int64(30), np.float64(30.0): np.int64(29), np.float64(31.0): np.int64(28), np.float64(32.0): np.int64(34), np.float64(33.0): np.int64(28), np.float64(34.0): np.int64(27), np.float64(35.0): np.int64(35), np.float64(36.0): np.int64(28), np.float64(37.0): np.int64(36), np.float64(38.0): np.int64(39), np.float64(39.0): np.int64(26), np.float64(40.0): np.int64(34), np.float64(41.0): np.int64(24), np.float64(42.0): np.int64(32), np.float64(43.0): np.int64(30), np.float64(44.0): np.int64(30), np.float64(45.0): np.int64(36), np.float64(46.0): np.int64(25), np.float64(47.0): np.int64(30), np.float64(48.0): np.int64(26), np.float64(49.0): np.int64(31), np.float64(50.0): np.int64(261), np.float64(56.0): np.int64(279), np.float64(57.0): np.int64(278), np.float64(58.0): np.int64(267), np.float64(59.0): np.int64(242), np.float64(60.0): np.int64(256), np.float64(61.0): np.int64(240), np.float64(62.0): np.int64(209), np.float64(63.0): np.int64(238), np.float64(64.0): np.int64(226), np.float64(65.0): np.int64(609), np.float64(66.0): np.int64(139), np.float64(67.0): np.int64(147), np.float64(68.0): np.int64(148), np.float64(69.0): np.int64(133), np.float64(70.0): np.int64(613), np.float64(71.0): np.int64(82), np.float64(72.0): np.int64(60), np.float64(73.0): np.int64(54), np.float64(74.0): np.int64(52), np.float64(75.0): np.int64(408), np.float64(76.0): np.int64(37), np.float64(77.0): np.int64(29), np.float64(78.0): np.int64(26), np.float64(79.0): np.int64(25), np.float64(80.0): np.int64(210), np.float64(81.0): np.int64(107), np.float64(82.0): np.int64(98), np.float64(83.0): np.int64(107)}\n",
      "Balanced training data shapes: X=(625, 12, 12), y=(625,). New counts: {np.float64(0.0): np.int64(5), np.float64(1.0): np.int64(8), np.float64(2.0): np.int64(8), np.float64(3.0): np.int64(8), np.float64(4.0): np.int64(8), np.float64(5.0): np.int64(8), np.float64(6.0): np.int64(4), np.float64(7.0): np.int64(8), np.float64(8.0): np.int64(8), np.float64(9.0): np.int64(8), np.float64(10.0): np.int64(8), np.float64(11.0): np.int64(8), np.float64(12.0): np.int64(8), np.float64(13.0): np.int64(8), np.float64(14.0): np.int64(8), np.float64(15.0): np.int64(8), np.float64(16.0): np.int64(8), np.float64(17.0): np.int64(8), np.float64(18.0): np.int64(8), np.float64(19.0): np.int64(8), np.float64(20.0): np.int64(8), np.float64(21.0): np.int64(8), np.float64(22.0): np.int64(8), np.float64(23.0): np.int64(8), np.float64(24.0): np.int64(8), np.float64(25.0): np.int64(8), np.float64(26.0): np.int64(8), np.float64(27.0): np.int64(8), np.float64(28.0): np.int64(8), np.float64(29.0): np.int64(8), np.float64(30.0): np.int64(8), np.float64(31.0): np.int64(8), np.float64(32.0): np.int64(8), np.float64(33.0): np.int64(8), np.float64(34.0): np.int64(8), np.float64(35.0): np.int64(8), np.float64(36.0): np.int64(8), np.float64(37.0): np.int64(8), np.float64(38.0): np.int64(8), np.float64(39.0): np.int64(8), np.float64(40.0): np.int64(8), np.float64(41.0): np.int64(8), np.float64(42.0): np.int64(8), np.float64(43.0): np.int64(8), np.float64(44.0): np.int64(8), np.float64(45.0): np.int64(8), np.float64(46.0): np.int64(8), np.float64(47.0): np.int64(8), np.float64(48.0): np.int64(8), np.float64(49.0): np.int64(8), np.float64(50.0): np.int64(8), np.float64(56.0): np.int64(8), np.float64(57.0): np.int64(8), np.float64(58.0): np.int64(8), np.float64(59.0): np.int64(8), np.float64(60.0): np.int64(8), np.float64(61.0): np.int64(8), np.float64(62.0): np.int64(8), np.float64(63.0): np.int64(8), np.float64(64.0): np.int64(8), np.float64(65.0): np.int64(8), np.float64(66.0): np.int64(8), np.float64(67.0): np.int64(8), np.float64(68.0): np.int64(8), np.float64(69.0): np.int64(8), np.float64(70.0): np.int64(8), np.float64(71.0): np.int64(8), np.float64(72.0): np.int64(8), np.float64(73.0): np.int64(8), np.float64(74.0): np.int64(8), np.float64(75.0): np.int64(8), np.float64(76.0): np.int64(8), np.float64(77.0): np.int64(8), np.float64(78.0): np.int64(8), np.float64(79.0): np.int64(8), np.float64(80.0): np.int64(8), np.float64(81.0): np.int64(8), np.float64(82.0): np.int64(8), np.float64(83.0): np.int64(8)}\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = create_sequences_for_classification(\n",
    "    df=train_df, \n",
    "    target_column_name=TARGET_COLUMN, \n",
    "    numerical_features_cols=NUMERICAL_COLS, \n",
    "    categorical_features_cols=CATEGORICAL_COLS, \n",
    "    prev_time_steps=SEQUENCE_LENGTH,\n",
    "    balance_data=True,\n",
    "    balance_random_seed=42,\n",
    "    # split_date=pd.Timestamp(year=2022, month=1, day=1)\n",
    "    split_date=pd.Timestamp(year=2013, month=1, day=1),\n",
    "    max_other_ratio=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "301a6f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training X shape: (625, 12, 12)\n",
      "Final Training y shape: (625,)\n",
      "Training y distribution: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
      "       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n",
      "       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n",
      "       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 56.,\n",
      "       57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69.,\n",
      "       70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82.,\n",
      "       83.]), array([5, 8, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]))\n",
      "Final Test X shape: (157, 12, 12)\n",
      "Final Test y shape: (157,)\n",
      "Test y distribution: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
      "       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n",
      "       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n",
      "       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 56.,\n",
      "       57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69.,\n",
      "       70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82.,\n",
      "       83.]), array([2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))\n"
     ]
    }
   ],
   "source": [
    "if train_data and train_data[\"X\"].size > 0:\n",
    "    print(f\"Final Training X shape: {train_data['X'].shape}\")\n",
    "    print(f\"Final Training y shape: {train_data['y'].shape}\")\n",
    "    y_int = train_data['y']\n",
    "    print(f\"Training y distribution: {np.unique(y_int, equal_nan=True, return_counts=True)}\")\n",
    "else:\n",
    "    print(\"Training data is empty or could not be generated.\")\n",
    "\n",
    "if test_data and test_data[\"X\"].size > 0:\n",
    "    print(f\"Final Test X shape: {test_data['X'].shape}\")\n",
    "    print(f\"Final Test y shape: {test_data['y'].shape}\")\n",
    "    y_int = test_data['y']\n",
    "    print(f\"Test y distribution: {np.unique(y_int, equal_nan=True, return_counts=True)}\")\n",
    "else:\n",
    "    print(\"Test data is empty or could not be generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec62da4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing all data... Original counts: {np.float64(0.0): np.int64(109), np.float64(1.0): np.int64(411), np.float64(2.0): np.int64(264), np.float64(3.0): np.int64(163), np.float64(4.0): np.int64(88), np.float64(5.0): np.int64(65), np.float64(6.0): np.int64(54), np.float64(7.0): np.int64(61), np.float64(8.0): np.int64(49), np.float64(9.0): np.int64(42), np.float64(10.0): np.int64(40), np.float64(11.0): np.int64(60), np.float64(12.0): np.int64(55), np.float64(13.0): np.int64(54), np.float64(14.0): np.int64(48), np.float64(15.0): np.int64(51), np.float64(16.0): np.int64(60), np.float64(17.0): np.int64(57), np.float64(18.0): np.int64(59), np.float64(19.0): np.int64(66), np.float64(20.0): np.int64(64), np.float64(21.0): np.int64(76), np.float64(22.0): np.int64(73), np.float64(23.0): np.int64(72), np.float64(24.0): np.int64(96), np.float64(25.0): np.int64(73), np.float64(26.0): np.int64(64), np.float64(27.0): np.int64(72), np.float64(28.0): np.int64(92), np.float64(29.0): np.int64(88), np.float64(30.0): np.int64(94), np.float64(31.0): np.int64(79), np.float64(32.0): np.int64(78), np.float64(33.0): np.int64(84), np.float64(34.0): np.int64(94), np.float64(35.0): np.int64(93), np.float64(36.0): np.int64(92), np.float64(37.0): np.int64(75), np.float64(38.0): np.int64(82), np.float64(39.0): np.int64(80), np.float64(40.0): np.int64(75), np.float64(41.0): np.int64(80), np.float64(42.0): np.int64(74), np.float64(43.0): np.int64(80), np.float64(44.0): np.int64(70), np.float64(45.0): np.int64(85), np.float64(46.0): np.int64(70), np.float64(47.0): np.int64(94), np.float64(48.0): np.int64(76), np.float64(49.0): np.int64(70), np.float64(50.0): np.int64(922), np.float64(56.0): np.int64(862), np.float64(57.0): np.int64(896), np.float64(58.0): np.int64(1009), np.float64(59.0): np.int64(1060), np.float64(60.0): np.int64(1150), np.float64(61.0): np.int64(1157), np.float64(62.0): np.int64(1146), np.float64(63.0): np.int64(1192), np.float64(64.0): np.int64(1103), np.float64(65.0): np.int64(2454), np.float64(66.0): np.int64(793), np.float64(67.0): np.int64(836), np.float64(68.0): np.int64(859), np.float64(69.0): np.int64(831), np.float64(70.0): np.int64(4249), np.float64(71.0): np.int64(484), np.float64(72.0): np.int64(531), np.float64(73.0): np.int64(489), np.float64(74.0): np.int64(439), np.float64(75.0): np.int64(4402), np.float64(76.0): np.int64(303), np.float64(77.0): np.int64(275), np.float64(78.0): np.int64(306), np.float64(79.0): np.int64(291), np.float64(80.0): np.int64(2269), np.float64(81.0): np.int64(919), np.float64(82.0): np.int64(690), np.float64(83.0): np.int64(869)}\n",
      "Balanced data shapes: X=(3160, 12, 12), y=(3160,). New counts: {np.float64(0.0): np.int64(40), np.float64(1.0): np.int64(40), np.float64(2.0): np.int64(40), np.float64(3.0): np.int64(40), np.float64(4.0): np.int64(40), np.float64(5.0): np.int64(40), np.float64(6.0): np.int64(40), np.float64(7.0): np.int64(40), np.float64(8.0): np.int64(40), np.float64(9.0): np.int64(40), np.float64(10.0): np.int64(40), np.float64(11.0): np.int64(40), np.float64(12.0): np.int64(40), np.float64(13.0): np.int64(40), np.float64(14.0): np.int64(40), np.float64(15.0): np.int64(40), np.float64(16.0): np.int64(40), np.float64(17.0): np.int64(40), np.float64(18.0): np.int64(40), np.float64(19.0): np.int64(40), np.float64(20.0): np.int64(40), np.float64(21.0): np.int64(40), np.float64(22.0): np.int64(40), np.float64(23.0): np.int64(40), np.float64(24.0): np.int64(40), np.float64(25.0): np.int64(40), np.float64(26.0): np.int64(40), np.float64(27.0): np.int64(40), np.float64(28.0): np.int64(40), np.float64(29.0): np.int64(40), np.float64(30.0): np.int64(40), np.float64(31.0): np.int64(40), np.float64(32.0): np.int64(40), np.float64(33.0): np.int64(40), np.float64(34.0): np.int64(40), np.float64(35.0): np.int64(40), np.float64(36.0): np.int64(40), np.float64(37.0): np.int64(40), np.float64(38.0): np.int64(40), np.float64(39.0): np.int64(40), np.float64(40.0): np.int64(40), np.float64(41.0): np.int64(40), np.float64(42.0): np.int64(40), np.float64(43.0): np.int64(40), np.float64(44.0): np.int64(40), np.float64(45.0): np.int64(40), np.float64(46.0): np.int64(40), np.float64(47.0): np.int64(40), np.float64(48.0): np.int64(40), np.float64(49.0): np.int64(40), np.float64(50.0): np.int64(40), np.float64(56.0): np.int64(40), np.float64(57.0): np.int64(40), np.float64(58.0): np.int64(40), np.float64(59.0): np.int64(40), np.float64(60.0): np.int64(40), np.float64(61.0): np.int64(40), np.float64(62.0): np.int64(40), np.float64(63.0): np.int64(40), np.float64(64.0): np.int64(40), np.float64(65.0): np.int64(40), np.float64(66.0): np.int64(40), np.float64(67.0): np.int64(40), np.float64(68.0): np.int64(40), np.float64(69.0): np.int64(40), np.float64(70.0): np.int64(40), np.float64(71.0): np.int64(40), np.float64(72.0): np.int64(40), np.float64(73.0): np.int64(40), np.float64(74.0): np.int64(40), np.float64(75.0): np.int64(40), np.float64(76.0): np.int64(40), np.float64(77.0): np.int64(40), np.float64(78.0): np.int64(40), np.float64(79.0): np.int64(40), np.float64(80.0): np.int64(40), np.float64(81.0): np.int64(40), np.float64(82.0): np.int64(40), np.float64(83.0): np.int64(40)}\n"
     ]
    }
   ],
   "source": [
    "val_data = create_sequences_for_classification(\n",
    "    df=test_df, \n",
    "    target_column_name=TARGET_COLUMN, \n",
    "    numerical_features_cols=NUMERICAL_COLS, \n",
    "    categorical_features_cols=CATEGORICAL_COLS, \n",
    "    prev_time_steps=SEQUENCE_LENGTH,\n",
    "    balance_data=True,\n",
    "    balance_random_seed=42,\n",
    "    # split_date=pd.Timestamp(year=2022, month=1, day=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62514dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 85)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv_encoder.transform([[0]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f2a96",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f9ac0",
   "metadata": {},
   "source": [
    "## Raindrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efaf9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:53:34 [INFO]: No given device, using default device: cuda\n",
      "2025-06-03 20:53:34 [INFO]: Model files will be saved to ./runs/classify/WEATHER-KNMI/raindrop/20250603_T205334\n",
      "2025-06-03 20:53:34 [INFO]: Tensorboard file will be saved to ./runs/classify/WEATHER-KNMI/raindrop/20250603_T205334/tensorboard\n",
      "2025-06-03 20:53:34 [INFO]: Using customized CrossEntropy as the training loss function.\n",
      "2025-06-03 20:53:34 [INFO]: Using customized CrossEntropy as the validation metric function.\n",
      "/home/next/magisterka/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/home/next/magisterka/.venv/lib/python3.11/site-packages/pypots/nn/modules/raindrop/backbone.py:114: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  nn.init.xavier_uniform(self.R_u)  # xavier_uniform also known as glorot\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m raindrop = \u001b[43mRaindrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# n_classes=vv_encoder.transform([[0]]).shape[1],\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_ffn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43msaving_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./runs/classify/WEATHER-KNMI/raindrop\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_saving_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/pypots/classification/raindrop/model.py:178\u001b[39m, in \u001b[36mRaindrop.__init__\u001b[39m\u001b[34m(self, n_steps, n_features, n_classes, n_layers, d_model, n_heads, d_ffn, dropout, d_static, aggregation, sensor_wise_mask, static, batch_size, epochs, patience, training_loss, validation_metric, optimizer, num_workers, device, saving_path, model_saving_strategy, verbose)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# set up the model\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28mself\u001b[39m.model = _Raindrop(\n\u001b[32m    163\u001b[39m     n_features=n_features,\n\u001b[32m    164\u001b[39m     n_layers=n_layers,\n\u001b[32m   (...)\u001b[39m\u001b[32m    176\u001b[39m     validation_metric=\u001b[38;5;28mself\u001b[39m.validation_metric,\n\u001b[32m    177\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_model_to_given_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28mself\u001b[39m._print_model_size()\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# set up the optimizer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/pypots/base.py:229\u001b[39m, in \u001b[36mBaseModel._send_model_to_given_device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    227\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel has been allocated to the given multiple devices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "raindrop = Raindrop(\n",
    "    n_steps=train_data['X'].shape[1],\n",
    "    n_features=train_data['X'].shape[2],\n",
    "    # n_classes=vv_encoder.transform([[0]]).shape[1],\n",
    "    n_classes=2,\n",
    "    n_layers=2,\n",
    "    d_model=train_data['X'].shape[2] * 4,\n",
    "    d_ffn=256,\n",
    "    n_heads=2,\n",
    "    dropout=0.3,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    patience=3,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=None,\n",
    "    saving_path='./runs/classify/WEATHER-KNMI/raindrop',\n",
    "    model_saving_strategy='best',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84ce0112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5514, dtype=torch.float64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(\n",
    "    torch.tensor([[1.0, 0, 0], [0, 1, 0]], dtype=torch.float64), \n",
    "    torch.tensor([1,2])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68d36063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "2025-06-03 20:52:09 [ERROR]: ❌ Exception: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training got interrupted. Model was not trained. Please investigate the error printed above.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/pypots/base.py:739\u001b[39m, in \u001b[36mBaseNNModel._train_model\u001b[39m\u001b[34m(self, train_dataloader, val_dataloader)\u001b[39m\n\u001b[32m    738\u001b[39m loss = results[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m].sum()\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mraindrop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m results = raindrop.predict(test_data)\n\u001b[32m      3\u001b[39m prediction = results[\u001b[33m'\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/pypots/classification/raindrop/model.py:266\u001b[39m, in \u001b[36mRaindrop.fit\u001b[39m\u001b[34m(self, train_set, val_set, file_type)\u001b[39m\n\u001b[32m    258\u001b[39m     val_dataloader = DataLoader(\n\u001b[32m    259\u001b[39m         val_dataset,\n\u001b[32m    260\u001b[39m         batch_size=\u001b[38;5;28mself\u001b[39m.batch_size,\n\u001b[32m    261\u001b[39m         shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    262\u001b[39m         num_workers=\u001b[38;5;28mself\u001b[39m.num_workers,\n\u001b[32m    263\u001b[39m     )\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# Step 2: train the model and freeze it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28mself\u001b[39m.model.load_state_dict(\u001b[38;5;28mself\u001b[39m.best_model_dict)\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# Step 3: save the model if necessary\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/magisterka/.venv/lib/python3.11/site-packages/pypots/base.py:814\u001b[39m, in \u001b[36mBaseNNModel._train_model\u001b[39m\u001b[34m(self, train_dataloader, val_dataloader)\u001b[39m\n\u001b[32m    812\u001b[39m logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m❌ Exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.best_model_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# if no best model, raise error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    815\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTraining got interrupted. Model was not trained. Please investigate the error printed above.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    816\u001b[39m     )\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    818\u001b[39m     \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m(\n\u001b[32m    819\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTraining got interrupted. Please investigate the error printed above.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    820\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mModel got trained and will load the best checkpoint so far for testing.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    821\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt want it, please try fit() again.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    822\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Training got interrupted. Model was not trained. Please investigate the error printed above."
     ]
    }
   ],
   "source": [
    "raindrop.fit(train_set=train_data, val_set=test_data)\n",
    "results = raindrop.predict(test_data)\n",
    "prediction = results['classification']\n",
    "metrics = calc_binary_classification_metrics(prediction, test_data['y'])\n",
    "print(\"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    "    f'Accuracy: {metrics[\"accuracy\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf20327",
   "metadata": {},
   "source": [
    "## BRITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0847e160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 21:42:12 [INFO]: No given device, using default device: cuda\n",
      "2025-05-21 21:42:12 [INFO]: Model files will be saved to ./runs/classify/WEATHER-KNMI/brits/20250521_T214212\n",
      "2025-05-21 21:42:12 [INFO]: Tensorboard file will be saved to ./runs/classify/WEATHER-KNMI/brits/20250521_T214212/tensorboard\n",
      "2025-05-21 21:42:12 [INFO]: Using customized CrossEntropy as the training loss function.\n",
      "2025-05-21 21:42:12 [INFO]: Using customized CrossEntropy as the validation metric function.\n",
      "2025-05-21 21:42:12 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 592,612\n"
     ]
    }
   ],
   "source": [
    "brits = BRITS(\n",
    "    n_steps=train_data['X'].shape[1],\n",
    "    n_features=train_data['X'].shape[2],\n",
    "    n_classes=2,\n",
    "    rnn_hidden_size=256,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    patience=3,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=None,\n",
    "    saving_path='./runs/classify/WEATHER-KNMI/brits',\n",
    "    model_saving_strategy='best'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "735b1014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 21:42:32 [INFO]: Epoch 001 - training loss (CrossEntropy): 2901.0337, validation CrossEntropy: 0.6801\n",
      "2025-05-21 21:42:51 [INFO]: Epoch 002 - training loss (CrossEntropy): 2373.2363, validation CrossEntropy: 0.6686\n",
      "2025-05-21 21:43:06 [INFO]: Epoch 003 - training loss (CrossEntropy): 2074.8020, validation CrossEntropy: 0.6677\n",
      "2025-05-21 21:43:21 [INFO]: Epoch 004 - training loss (CrossEntropy): 1931.4504, validation CrossEntropy: 0.6531\n",
      "2025-05-21 21:43:37 [INFO]: Epoch 005 - training loss (CrossEntropy): 1874.9596, validation CrossEntropy: 0.6409\n",
      "2025-05-21 21:43:58 [INFO]: Epoch 006 - training loss (CrossEntropy): 1862.3724, validation CrossEntropy: 0.6391\n",
      "2025-05-21 21:44:17 [INFO]: Epoch 007 - training loss (CrossEntropy): 1855.3852, validation CrossEntropy: 0.6266\n",
      "2025-05-21 21:44:36 [INFO]: Epoch 008 - training loss (CrossEntropy): 1848.6624, validation CrossEntropy: 0.6175\n",
      "2025-05-21 21:44:54 [INFO]: Epoch 009 - training loss (CrossEntropy): 1842.3058, validation CrossEntropy: 0.6124\n",
      "2025-05-21 21:45:14 [INFO]: Epoch 010 - training loss (CrossEntropy): 1836.1539, validation CrossEntropy: 0.6210\n",
      "2025-05-21 21:45:34 [INFO]: Epoch 011 - training loss (CrossEntropy): 1830.1355, validation CrossEntropy: 0.6004\n",
      "2025-05-21 21:45:54 [INFO]: Epoch 012 - training loss (CrossEntropy): 1824.3533, validation CrossEntropy: 0.6088\n",
      "2025-05-21 21:46:21 [INFO]: Epoch 013 - training loss (CrossEntropy): 1818.7659, validation CrossEntropy: 0.5916\n",
      "2025-05-21 21:47:09 [INFO]: Epoch 014 - training loss (CrossEntropy): 1813.3129, validation CrossEntropy: 0.5872\n",
      "2025-05-21 21:47:27 [INFO]: Epoch 015 - training loss (CrossEntropy): 1807.9233, validation CrossEntropy: 0.5779\n",
      "2025-05-21 21:47:48 [INFO]: Epoch 016 - training loss (CrossEntropy): 1802.5182, validation CrossEntropy: 0.5759\n",
      "2025-05-21 21:48:11 [INFO]: Epoch 017 - training loss (CrossEntropy): 1797.1697, validation CrossEntropy: 0.5710\n",
      "2025-05-21 21:48:34 [INFO]: Epoch 018 - training loss (CrossEntropy): 1791.7170, validation CrossEntropy: 0.5715\n",
      "2025-05-21 21:48:58 [INFO]: Epoch 019 - training loss (CrossEntropy): 1786.3215, validation CrossEntropy: 0.5800\n",
      "2025-05-21 21:49:22 [INFO]: Epoch 020 - training loss (CrossEntropy): 1780.8330, validation CrossEntropy: 0.5645\n",
      "2025-05-21 21:49:22 [INFO]: Finished training. The best model is from epoch#20.\n",
      "2025-05-21 21:49:22 [INFO]: Saved the model to ./runs/classify/WEATHER-KNMI/brits/20250521_T214212/BRITS.pypots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.9007518796992481, \n",
      "PR_AUC: 0.9232825513565572,\n",
      "F1: 0.9023668639053254,\n",
      "Precision: 0.8879184861717613,\n",
      "Recall: 0.9172932330827067,\n",
      "Accuracy: 0.9007518796992481\n"
     ]
    }
   ],
   "source": [
    "brits.fit(train_set=train_data, val_set=test_data)\n",
    "results = brits.predict(test_data)\n",
    "prediction = results['classification']\n",
    "metrics = calc_binary_classification_metrics(prediction, test_data['y'])\n",
    "print(\"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    "    f'Accuracy: {metrics[\"accuracy\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b8222",
   "metadata": {},
   "source": [
    "## Not deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "133ce0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import ClassifierMixin\n",
    "from typing import Any, TypeVar\n",
    "from collections import namedtuple\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ba30246",
   "metadata": {},
   "outputs": [],
   "source": [
    "_T = TypeVar('_T', bound=ClassifierMixin)\n",
    "\n",
    "def evaluate_model(model: ClassifierMixin, X: Any, y: Any):\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred, average='macro')\n",
    "    precision = precision_score(y, y_pred, average='macro')\n",
    "    recall = recall_score(y, y_pred, average='macro')\n",
    "    confusion = confusion_matrix(y, y_pred)\n",
    "    return namedtuple('Evaluation', ['accuracy', 'f1', 'precision', 'recall', 'confusion'])(accuracy, f1, precision, recall, confusion)\n",
    "\n",
    "def train_model(\n",
    "        model_cls: _T, \n",
    "        model_kwargs: dict[str, Any],\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        X_test: np.ndarray,\n",
    "        y_test: np.ndarray\n",
    "    ) -> _T:\n",
    "    model = model_cls(**model_kwargs)\n",
    "    if X_train.ndim == 3:\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "    if X_test.ndim == 3:\n",
    "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1] * X_test.shape[2]))\n",
    "\n",
    "    X_train = np.nan_to_num(X_train, nan=-1.0)\n",
    "    X_test = np.nan_to_num(X_test, nan=-1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_metrics = evaluate_model(model, X_train, y_train)\n",
    "    val_metrics = evaluate_model(model, X_test, y_test)\n",
    "    print(f\"Model - {model_cls.__name__}\")\n",
    "    print(\"\\tTrain metrics:\")\n",
    "    print(f\"\\t\\tAccuracy: {train_metrics.accuracy:.4f}\")\n",
    "    print(f\"\\t\\tF1: {train_metrics.f1:.4f}\")\n",
    "    print(f\"\\t\\tPrecision: {train_metrics.precision:.4f}\")\n",
    "    print(f\"\\t\\tRecall: {train_metrics.recall:.4f}\")\n",
    "    print(\"\\tValidation metrics:\")\n",
    "    print(f\"\\t\\tAccuracy: {val_metrics.accuracy:.4f}\")\n",
    "    print(f\"\\t\\tF1: {val_metrics.f1:.4f}\")\n",
    "    print(f\"\\t\\tPrecision: {val_metrics.precision:.4f}\")\n",
    "    print(f\"\\t\\tRecall: {val_metrics.recall:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "891be840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - SVC\n",
      "\tTrain metrics:\n",
      "\t\tAccuracy: 0.6802\n",
      "\t\tF1: 0.6641\n",
      "\t\tPrecision: 0.7228\n",
      "\t\tRecall: 0.6802\n",
      "\tValidation metrics:\n",
      "\t\tAccuracy: 0.6857\n",
      "\t\tF1: 0.6606\n",
      "\t\tPrecision: 0.7639\n",
      "\t\tRecall: 0.6857\n"
     ]
    }
   ],
   "source": [
    "svc = train_model(\n",
    "    SVC,\n",
    "    {},\n",
    "    X_train=train_data[\"X\"],\n",
    "    y_train=train_data[\"y\"],\n",
    "    X_test=test_data[\"X\"],\n",
    "    y_test=test_data[\"y\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "973810c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - RandomForestClassifier\n",
      "\tTrain metrics:\n",
      "\t\tAccuracy: 1.0000\n",
      "\t\tF1: 1.0000\n",
      "\t\tPrecision: 1.0000\n",
      "\t\tRecall: 1.0000\n",
      "\tValidation metrics:\n",
      "\t\tAccuracy: 0.9459\n",
      "\t\tF1: 0.9459\n",
      "\t\tPrecision: 0.9461\n",
      "\t\tRecall: 0.9459\n"
     ]
    }
   ],
   "source": [
    "rfc = train_model(\n",
    "    RandomForestClassifier,\n",
    "    {},\n",
    "    X_train=train_data[\"X\"],\n",
    "    y_train=train_data[\"y\"],\n",
    "    X_test=test_data[\"X\"],\n",
    "    y_test=test_data[\"y\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f42a7d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - XGBClassifier\n",
      "\tTrain metrics:\n",
      "\t\tAccuracy: 1.0000\n",
      "\t\tF1: 1.0000\n",
      "\t\tPrecision: 1.0000\n",
      "\t\tRecall: 1.0000\n",
      "\tValidation metrics:\n",
      "\t\tAccuracy: 0.9308\n",
      "\t\tF1: 0.9307\n",
      "\t\tPrecision: 0.9335\n",
      "\t\tRecall: 0.9308\n"
     ]
    }
   ],
   "source": [
    "xgb = train_model(\n",
    "    XGBClassifier,\n",
    "    {},\n",
    "    X_train=train_data[\"X\"],\n",
    "    y_train=train_data[\"y\"],\n",
    "    X_test=test_data[\"X\"],\n",
    "    y_test=test_data[\"y\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
